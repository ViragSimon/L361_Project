{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Velocity demo\n",
    "Current approach shares full optimizer state between rounds. Optimizer states are aggregated when received on server end and then shared out such that all clients begin the next round with the aggregated optimizer state.\n",
    "Limitations: \n",
    "- We are not measuring communication from this, but if desired, it should be pretty easy to add\n",
    "- Currently, we share the entire optimizer state, but when training only a single layer, we would only need to share the state related to that layer. To only share updated layer, modify _aggregate_optimizer_states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple, Union, Callable\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import sys\n",
    "import base64\n",
    "import pickle\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import DataLoader\n",
    "from flwr.server.strategy import Strategy\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Metrics, Context, Status, GetParametersRes, Parameters, GetParametersIns, MetricsAggregationFn,NDArrays,Scalar\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents \n",
    "from flwr.server.strategy import FedAvg, FedProx\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.strategy.aggregate import aggregate, weighted_loss_avg\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def load_datasets(partition_id, num_partitions: int):\n",
    "    fds = FederatedDataset(dataset=\"cifar10\", partitioners={\"train\": num_partitions})\n",
    "    partition = fds.load_partition(partition_id)\n",
    "    # Divide data on each node: 80% train, 20% test\n",
    "    partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
    "    pytorch_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        \n",
    "        batch[\"img\"] = [pytorch_transforms(img) for img in batch[\"img\"]]\n",
    "        return batch\n",
    "\n",
    "    partition_train_test = partition_train_test.with_transform(apply_transforms)\n",
    "    trainloader = DataLoader(partition_train_test[\"train\"], batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(partition_train_test[\"test\"], batch_size=32)\n",
    "    testset = fds.load_split(\"test\").with_transform(apply_transforms)\n",
    "    testloader = DataLoader(testset, batch_size=32)\n",
    "    return trainloader, valloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def train(net, trainloader, epochs: int, optimizer=None):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    print(f\"training network...\")\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for batch in trainloader:\n",
    "            images, labels = batch[\"img\"], batch[\"label\"]\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images, labels = batch[\"img\"], batch[\"label\"]\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy\n",
    "\n",
    "def freeze_layers(model: torch.nn.Module, trainable_layers: int) -> None:\n",
    "        \"\"\"Freeze specified layers of the model.\"\"\"\n",
    "        for idx, (name, param) in enumerate(model.named_parameters()):\n",
    "            if idx == trainable_layers or trainable_layers == -1:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "def get_parameters_size(params: Parameters) -> int:\n",
    "    size = sys.getsizeof(params)  # Base size of the dataclass instance\n",
    "    size += sys.getsizeof(params.tensor_type)  # Size of the string\n",
    "    size += sys.getsizeof(params.tensors)  # Size of the list container\n",
    "    size += sum(sys.getsizeof(tensor) for tensor in params.tensors)  # Size of each bytes object\n",
    "    return size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NETWORK_LEN = len(Net().state_dict().keys())\n",
    "EPOCHS = 5\n",
    "NUM_PARTITIONS = 3\n",
    "NUM_OF_CYCLES  = 1\n",
    "NUM_OF_FULL_UPDATES_BETWEEN_CYCLES = 5\n",
    "NUM_OF_ROUNDS = (NUM_OF_CYCLES * NUM_OF_FULL_UPDATES_BETWEEN_CYCLES) + (NUM_OF_CYCLES * NETWORK_LEN *2)\n",
    "print(f\"Number of rounds: {NUM_OF_ROUNDS}\")\n",
    "backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.common import NDArrays, Scalar\n",
    "\n",
    "def get_evaluate_fn(\n",
    "    testloader: DataLoader,\n",
    "    net: torch.nn.Module,\n",
    ") -> Callable[[int, NDArrays, Dict[str, Scalar]], Optional[Tuple[float, Dict[str, Scalar]]]]:\n",
    "    \"\"\"Return an evaluation function for server-side evaluation.\"\"\"\n",
    "\n",
    "    def evaluate(\n",
    "        server_round: int, parameters: NDArrays, config: Dict[str, Scalar]\n",
    "    ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "        \"\"\"Use the entire test set for evaluation.\"\"\"\n",
    "        \n",
    "        # Copy model parameters to avoid modifying the original\n",
    "        net_copy = copy.deepcopy(net)\n",
    "        \n",
    "        # Update model with the latest parameters\n",
    "        params_dict = zip(net_copy.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "        net_copy.load_state_dict(state_dict, strict=True)\n",
    "        \n",
    "        net_copy.to(DEVICE)\n",
    "        net_copy.eval()\n",
    "\n",
    "        # Test the model\n",
    "        loss, accuracy = test(net_copy, testloader)\n",
    "        \n",
    "        # Return loss and metrics\n",
    "        return loss, {\"accuracy\": accuracy}\n",
    "\n",
    "    return evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedAvg with velocity from global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.strategy.aggregate import aggregate, weighted_loss_avg\n",
    "\n",
    "fed_part_avg_result = {}\n",
    "fed_part_avg_model_results = {}\n",
    "\n",
    "class FedPartAvgWithVelocity(Strategy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fraction_fit: float = 1.0,\n",
    "        fraction_evaluate: float = 1.0,\n",
    "        min_fit_clients: int = 2,\n",
    "        min_evaluate_clients: int = 2,\n",
    "        min_available_clients: int = 2,\n",
    "        evaluate_fn: Optional[\n",
    "            Callable[\n",
    "                [int, NDArrays, dict[str, Scalar]],\n",
    "                Optional[tuple[float, dict[str, Scalar]]],\n",
    "            ]\n",
    "        ] = None,\n",
    "        on_fit_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,\n",
    "        on_evaluate_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,\n",
    "        accept_failures: bool = True,\n",
    "        initial_parameters: Optional[Parameters] = None,\n",
    "        fit_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
    "        evaluate_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
    "        inplace: bool = True,\n",
    "        layer_update_strategy: str = \"sequential\",\n",
    "        \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.fraction_fit = fraction_fit\n",
    "        self.fraction_evaluate = fraction_evaluate\n",
    "        self.min_fit_clients = min_fit_clients\n",
    "        self.min_evaluate_clients = min_evaluate_clients\n",
    "        self.min_available_clients = min_available_clients\n",
    "        self.evaluate_fn = evaluate_fn\n",
    "        self.on_fit_config_fn = on_fit_config_fn\n",
    "        self.on_evaluate_config_fn = on_evaluate_config_fn\n",
    "        self.accept_failures = accept_failures\n",
    "        self.initial_parameters = initial_parameters\n",
    "        self.fit_metrics_aggregation_fn = fit_metrics_aggregation_fn\n",
    "        self.evaluate_metrics_aggregation_fn = evaluate_metrics_aggregation_fn\n",
    "        self.inplace = inplace\n",
    "\n",
    "        self.layer_update_strategy = layer_update_strategy  # 'sequential' or 'cyclic'\n",
    "        self.current_layer = 0  # Track which layer to update\n",
    "        self.number_of_layers = None\n",
    "        self.layer_training_sequence = []\n",
    "        self.training_sequence_index = 0\n",
    "        self.latest_parameters = initial_parameters\n",
    "        self.global_optim_state = None\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"FedPartAvgWithVelocity\"\n",
    "    \n",
    "\n",
    "    def num_fit_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Return sample size and required number of clients.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_fit)\n",
    "        return max(num_clients, self.min_fit_clients), self.min_available_clients\n",
    "\n",
    "    def num_evaluation_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Use a fraction of available clients for evaluation.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_evaluate)\n",
    "        return max(num_clients, self.min_evaluate_clients), self.min_available_clients\n",
    "    \n",
    "    def generate_layer_training_sequence(self) -> List[int]:\n",
    "        \"\"\"Generate a sequence of layers to train.\"\"\"\n",
    "        layer_training_sequence = []\n",
    "        for _ in range(NUM_OF_CYCLES):\n",
    "            for _ in range(NUM_OF_FULL_UPDATES_BETWEEN_CYCLES):\n",
    "                    layer_training_sequence.append(-1)\n",
    "            for layer in range(NETWORK_LEN):\n",
    "                    layer_training_sequence.append(layer)\n",
    "                    layer_training_sequence.append(layer)\n",
    "\n",
    "        return layer_training_sequence\n",
    "\n",
    "    def initialize_parameters(\n",
    "        self, client_manager: ClientManager\n",
    "    ) -> Optional[Parameters]:\n",
    "        \"\"\"Initialize global model parameters.\"\"\"\n",
    "        net = Net()\n",
    "        ndarrays = get_parameters(net)\n",
    "        self.layer_training_sequence = self.generate_layer_training_sequence()\n",
    "        self.number_of_layers = len(ndarrays)\n",
    "\n",
    "        return ndarrays_to_parameters(ndarrays)\n",
    "    \n",
    "\n",
    "\n",
    "    def evaluate(\n",
    "        self, server_round: int, parameters: Parameters\n",
    "    ) -> Optional[tuple[float, dict[str, Scalar]]]:\n",
    "        \"\"\"Evaluate model parameters using an evaluation function.\"\"\"\n",
    "        if self.evaluate_fn is None:\n",
    "            # No evaluation function provided\n",
    "            return None\n",
    "        parameters_ndarrays = parameters_to_ndarrays(parameters)\n",
    "        eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})\n",
    "        if eval_res is None:\n",
    "            return None\n",
    "        \n",
    "        if server_round in fed_part_avg_model_results:  \n",
    "            expand_fed_part_avg_model_results= {**fed_part_avg_model_results[server_round], \"global_loss\": eval_res[0], \"global_metrics\": eval_res[1]}\n",
    "        else:\n",
    "            expand_fed_part_avg_model_results= {\"global_loss\": eval_res[0], \"global_metrics\": eval_res[1]}\n",
    "        \n",
    "        fed_part_avg_model_results[server_round] = expand_fed_part_avg_model_results\n",
    "        \n",
    "        loss, metrics = eval_res\n",
    "        return loss, metrics\n",
    "\n",
    "    def configure_fit(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, FitIns]]:\n",
    "        \"\"\"Configure the next round of training.\"\"\"\n",
    "        \n",
    "        config = {\"trainable_layers\": self.layer_training_sequence[self.training_sequence_index]}\n",
    "\n",
    "        if self.global_optim_state is not None:\n",
    "            config[\"optimizer_state\"] = self.global_optim_state\n",
    "        \n",
    "        sample_size, min_num_clients = self.num_fit_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "        \n",
    "        print(f\"Training on layer {self.layer_training_sequence}\")\n",
    "        fit_configurations = []\n",
    "        for idx, client in enumerate(clients):\n",
    "            fit_configurations.append((client, FitIns(parameters, config)))\n",
    "\n",
    "        self.training_sequence_index = self.training_sequence_index + 1\n",
    "        \n",
    "        return fit_configurations\n",
    "    \n",
    "\n",
    "    def configure_evaluate(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, EvaluateIns]]:\n",
    "        \"\"\"Configure the next round of evaluation.\"\"\"\n",
    "        if self.fraction_evaluate == 0.0:\n",
    "            return []\n",
    "        config = {}\n",
    "        evaluate_ins = EvaluateIns(parameters, config)\n",
    "\n",
    "        # Sample clients\n",
    "        sample_size, min_num_clients = self.num_evaluation_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "\n",
    "        # Return client/config pairs\n",
    "        return [(client, evaluate_ins) for client in clients]\n",
    "\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "        global global_all_optimizer_states\n",
    "        \"\"\"Aggregate fit results using weighted average.\"\"\"\n",
    "        \n",
    "        total_size = 0\n",
    "        for client, fit_res in results:\n",
    "            total_size += get_parameters_size(fit_res.parameters)\n",
    "        print(f\"total size: {total_size}\")\n",
    "        \n",
    "        weights_results = [\n",
    "            (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)\n",
    "            for _, fit_res in results\n",
    "        ]\n",
    "\n",
    "        aggregated_weights = aggregate(weights_results)\n",
    "        # parameters_aggregated = ndarrays_to_parameters(aggregate(weights_results))\n",
    "\n",
    "        if self.layer_training_sequence[self.training_sequence_index -1] == -1:\n",
    "            self.latest_parameters = ndarrays_to_parameters(aggregated_weights)\n",
    "        else:\n",
    "            current_model = parameters_to_ndarrays(self.latest_parameters)\n",
    "            current_model[self.layer_training_sequence[self.training_sequence_index -1]] = aggregated_weights[0]\n",
    "            self.latest_parameters = ndarrays_to_parameters(current_model)\n",
    "\n",
    "        metrics = {}\n",
    "        # include optimizer state in metrics\n",
    "\n",
    "        optimizer_states_serialized = [res.metrics.get(\"optimizer_state\", None) for _, res in results]\n",
    "        optimizer_states = [pickle.loads(base64.b64decode(state)) for state in optimizer_states_serialized if state is not None]\n",
    "\n",
    "        aggregated_optimizer_state = self._aggregate_optimizer_states(optimizer_states)\n",
    "        aggregated_optimizer_state_serialized = base64.b64encode(pickle.dumps(aggregated_optimizer_state)).decode('ascii')\n",
    "        metrics[\"optimizer_state\"] = aggregated_optimizer_state_serialized\n",
    "        self.global_optim_state = aggregated_optimizer_state_serialized\n",
    "\n",
    "        # Add sizes to total_size:\n",
    "        sizes = sum([sys.getsizeof(data) for data in optimizer_states_serialized if data is not None])\n",
    "        total_size += sizes\n",
    "\n",
    "        if fed_part_avg_result.get(server_round):\n",
    "            fed_part_avg_result[server_round][\"total_size\"] = total_size\n",
    "        else:\n",
    "            fed_part_avg_result[server_round] = {\"total_size\": total_size}\n",
    "\n",
    "        return self.latest_parameters, metrics\n",
    "\n",
    "    def _aggregate_optimizer_states(self, optimizer_states):\n",
    "        # aggregates by comuting (unweighted) mean of 1st and 2nd momentum, and taking the maximum step value. (alternatively, could be changed to reset steps to 0)\n",
    "        aggregated_state_dict = copy.deepcopy(optimizer_states[0])\n",
    "\n",
    "        for layer in range(len(optimizer_states[0]['state'])):\n",
    "            fst_momentums = []\n",
    "            snd_momentums = []\n",
    "            steps = []\n",
    "            for s in range(len(optimizer_states)):\n",
    "                fst_momentums.append(optimizer_states[s]['state'][layer]['exp_avg'])\n",
    "                snd_momentums.append(optimizer_states[s]['state'][layer]['exp_avg_sq'])\n",
    "                steps.append(optimizer_states[s]['state'][layer]['step'])\n",
    "            \n",
    "            mean_fst_momentum = torch.mean(torch.stack(fst_momentums), dim=0)\n",
    "            mean_snd_momentum = torch.mean(torch.stack(snd_momentums), dim=0)\n",
    "            max_step = torch.max(torch.stack(steps))\n",
    "\n",
    "            aggregated_state_dict['state'][layer]['exp_avg'][:] = mean_fst_momentum\n",
    "            aggregated_state_dict['state'][layer]['exp_avg_sq'][:] = mean_snd_momentum\n",
    "            aggregated_state_dict['state'][layer]['step'] = torch.tensor([max_step])\n",
    "        \n",
    "        return aggregated_state_dict\n",
    "\n",
    "\n",
    "    def aggregate_evaluate(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, EvaluateRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
    "    ) -> Tuple[Optional[float], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate evaluation losses using weighted average.\"\"\"\n",
    "\n",
    "        if not results:\n",
    "            return None, {}\n",
    "        \n",
    "        total_loss = 0\n",
    "        for _, evaluate_res in results:\n",
    "            total_loss += evaluate_res.loss\n",
    "\n",
    "        if fed_part_avg_result.get(server_round):\n",
    "            fed_part_avg_result[server_round][\"total_loss\"] = total_loss\n",
    "        else:\n",
    "            fed_part_avg_result[server_round] = {\"total_loss\": total_loss}\n",
    "\n",
    "        loss_aggregated = weighted_loss_avg(\n",
    "            [\n",
    "                (evaluate_res.num_examples, evaluate_res.loss)\n",
    "                for _, evaluate_res in results\n",
    "            ]\n",
    "        )\n",
    "        metrics_aggregated = {}\n",
    "        return loss_aggregated, metrics_aggregated\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedAvgPartWithVelocityFlowerClient(NumPyClient):\n",
    "    def __init__(self, partition_id, net, trainloader, valloader):\n",
    "        self.partition_id = partition_id\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.partition_id}] get_parameters\")\n",
    "        parameters = get_parameters(self.net)\n",
    "        if config[\"trainable_layers\"] == -1:\n",
    "            trained_layers = parameters\n",
    "        else:\n",
    "            trained_layers = [parameters[config[\"trainable_layers\"]]]\n",
    "        \n",
    "        return trained_layers\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        freeze_layers(self.net, config[\"trainable_layers\"])\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters())\n",
    "\n",
    "        if \"optimizer_state\" in config:\n",
    "            print(f\"Got optimizer state in config, setting..\")\n",
    "            serialized_optimizer_state = config[\"optimizer_state\"]\n",
    "            optim_state = pickle.loads(base64.b64decode(serialized_optimizer_state))\n",
    "            self._set_optimizer_state(optim_state)\n",
    "\n",
    "        train(self.net, self.trainloader, epochs=EPOCHS, optimizer=self.optimizer)\n",
    "        print('finished train')\n",
    "\n",
    "        # handle optimizer state (serialize before passing)\n",
    "        optim_state = self._get_optimizer_state()\n",
    "        serialized_optimizer_state = base64.b64encode(pickle.dumps(optim_state)).decode('ascii')\n",
    "        print(f\"After training, got optim state {serialized_optimizer_state[:50]}\")\n",
    "\n",
    "        return self.get_parameters(config), len(self.trainloader), {\"optimizer_state\": serialized_optimizer_state}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "    \n",
    "    def _get_optimizer_state(self):\n",
    "        state_dict = self.optimizer.state_dict()\n",
    "        return state_dict\n",
    "    \n",
    "    def _set_optimizer_state(self, state_dict):\n",
    "        self.optimizer.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def client_fn(context: Context) -> Client:\n",
    "    net = Net().to(DEVICE)\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "    trainloader, valloader, _ = load_datasets(partition_id, num_partitions)\n",
    "    return FedAvgPartWithVelocityFlowerClient(partition_id, net, trainloader, valloader).to_client()\n",
    "\n",
    "\n",
    "# Create the ClientApp\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(DEVICE)\n",
    "\n",
    "_, _, testloader = load_datasets(0, NUM_PARTITIONS)\n",
    "\n",
    "evaluate_fn = get_evaluate_fn(testloader, net)\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Configure the server for just 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=NUM_OF_ROUNDS)\n",
    "    return ServerAppComponents(\n",
    "        config=config,\n",
    "        strategy=FedPartAvgWithVelocity(\n",
    "            evaluate_fn=evaluate_fn,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "server = ServerApp(server_fn=server_fn)\n",
    "\n",
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_PARTITIONS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/fed_part_{EPOCHS}.p', 'wb') as file:\n",
    "    pickle.dump(fed_part_avg_result, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_run = 2\n",
    "with open(f'../data/fed_part_{load_run}.p', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Plot the total size of parameters for each round\n",
    "fed_part_avg_rounds = list(fed_part_avg_result.keys())\n",
    "fed_part_avg_sizes = [fed_part_avg_result[round][\"total_size\"] for round in fed_part_avg_rounds]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fed_part_avg_rounds, fed_part_avg_sizes, marker='o', linestyle='-', color='b', label='FedPartAvg')\n",
    "# plt.plot(fed_avg_rounds, fed_avg_sizes, marker='o', linestyle='-', color='r', label='FedAvg')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Total Size of Parameters (bytes)')\n",
    "plt.title('Total Size of Parameters for Each Round')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "fed_part_avg_losses = [fed_part_avg_result[round][\"total_loss\"] for round in fed_part_avg_rounds]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fed_part_avg_rounds, fed_part_avg_losses, marker='o', linestyle='-', color='b', label='FedPartAvg')\n",
    "# plt.plot(fed_avg_rounds, fed_avg_losses, marker='o', linestyle='-', color='r', label='FedAvg')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Total Loss for Each Round')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "fed_part_avg_model_rounds = list(fed_part_avg_model_results.keys())\n",
    "fed_part_avg_accuracies = [fed_part_avg_model_results[round][\"global_metrics\"][\"accuracy\"] for round in fed_part_avg_model_rounds]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fed_part_avg_model_rounds, fed_part_avg_accuracies, marker='o', linestyle='-', color='b', label='FedPartAvg')\n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_accuracies, marker='o', linestyle='-', color='r', label='FedAvg')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for Each Round')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "fed_part_avg_global_losses = [fed_part_avg_model_results[round][\"global_loss\"] for round in fed_part_avg_model_rounds]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fed_part_avg_model_rounds, fed_part_avg_global_losses, marker='o', linestyle='-', color='b', label='FedPartAvg')\n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_global_losses, marker='o', linestyle='-', color='r', label='FedAvg')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss for Each Round')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using momentum similarity for weighing updates\n",
    "Ideas:\n",
    "- Weigh updates by cosine similarity between client momentum and global momentum from previous round.\n",
    "- Save cosine similarities. Use these to compute the distributions of these similarities and identify \n",
    "- Use cosine similarities to identify outlier clients. \n",
    "\n",
    "\n",
    "This is likely of greater interest in heterogeneous data settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from functools import partial, reduce\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.strategy.aggregate import aggregate, weighted_loss_avg\n",
    "\n",
    "fed_momentum_weighting_results = {}\n",
    "fed_momentum_weighting_model_results = {}\n",
    "\n",
    "global client_opt\n",
    "global prev_opt\n",
    "\n",
    "class FedPartAvgWithVelocityweighting(Strategy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fraction_fit: float = 1.0,\n",
    "        fraction_evaluate: float = 1.0,\n",
    "        min_fit_clients: int = 2,\n",
    "        min_evaluate_clients: int = 2,\n",
    "        min_available_clients: int = 2,\n",
    "        evaluate_fn: Optional[\n",
    "            Callable[\n",
    "                [int, NDArrays, dict[str, Scalar]],\n",
    "                Optional[tuple[float, dict[str, Scalar]]],\n",
    "            ]\n",
    "        ] = None,\n",
    "        on_fit_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,\n",
    "        on_evaluate_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,\n",
    "        accept_failures: bool = True,\n",
    "        initial_parameters: Optional[Parameters] = None,\n",
    "        fit_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
    "        evaluate_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
    "        inplace: bool = True,\n",
    "        layer_update_strategy: str = \"sequential\",\n",
    "        \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.fraction_fit = fraction_fit\n",
    "        self.fraction_evaluate = fraction_evaluate\n",
    "        self.min_fit_clients = min_fit_clients\n",
    "        self.min_evaluate_clients = min_evaluate_clients\n",
    "        self.min_available_clients = min_available_clients\n",
    "        self.evaluate_fn = evaluate_fn\n",
    "        self.on_fit_config_fn = on_fit_config_fn\n",
    "        self.on_evaluate_config_fn = on_evaluate_config_fn\n",
    "        self.accept_failures = accept_failures\n",
    "        self.initial_parameters = initial_parameters\n",
    "        self.fit_metrics_aggregation_fn = fit_metrics_aggregation_fn\n",
    "        self.evaluate_metrics_aggregation_fn = evaluate_metrics_aggregation_fn\n",
    "        self.inplace = inplace\n",
    "\n",
    "        self.layer_update_strategy = layer_update_strategy  # 'sequential' or 'cyclic'\n",
    "        self.current_layer = 0  # Track which layer to update\n",
    "        self.number_of_layers = None\n",
    "        self.layer_training_sequence = []\n",
    "        self.training_sequence_index = 0\n",
    "        self.latest_parameters = initial_parameters\n",
    "        self.global_optim_state = None\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"FedPartAvgWithVelocity\"\n",
    "    \n",
    "\n",
    "    def num_fit_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Return sample size and required number of clients.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_fit)\n",
    "        return max(num_clients, self.min_fit_clients), self.min_available_clients\n",
    "\n",
    "    def num_evaluation_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Use a fraction of available clients for evaluation.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_evaluate)\n",
    "        return max(num_clients, self.min_evaluate_clients), self.min_available_clients\n",
    "    \n",
    "    def generate_layer_training_sequence(self) -> List[int]:\n",
    "        \"\"\"Generate a sequence of layers to train.\"\"\"\n",
    "        layer_training_sequence = []\n",
    "        for _ in range(NUM_OF_CYCLES):\n",
    "            for _ in range(NUM_OF_FULL_UPDATES_BETWEEN_CYCLES):\n",
    "                    layer_training_sequence.append(-1)\n",
    "            for layer in range(NETWORK_LEN):\n",
    "                    layer_training_sequence.append(layer)\n",
    "                    layer_training_sequence.append(layer)\n",
    "\n",
    "        return layer_training_sequence\n",
    "\n",
    "    def initialize_parameters(\n",
    "        self, client_manager: ClientManager\n",
    "    ) -> Optional[Parameters]:\n",
    "        \"\"\"Initialize global model parameters.\"\"\"\n",
    "        net = Net()\n",
    "        ndarrays = get_parameters(net)\n",
    "        self.layer_training_sequence = self.generate_layer_training_sequence()\n",
    "        self.number_of_layers = len(ndarrays)\n",
    "\n",
    "        return ndarrays_to_parameters(ndarrays)\n",
    "    \n",
    "\n",
    "\n",
    "    def evaluate(\n",
    "        self, server_round: int, parameters: Parameters\n",
    "    ) -> Optional[tuple[float, dict[str, Scalar]]]:\n",
    "        \"\"\"Evaluate model parameters using an evaluation function.\"\"\"\n",
    "        if self.evaluate_fn is None:\n",
    "            # No evaluation function provided\n",
    "            return None\n",
    "        parameters_ndarrays = parameters_to_ndarrays(parameters)\n",
    "        eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})\n",
    "        if eval_res is None:\n",
    "            return None\n",
    "        \n",
    "        if server_round in fed_momentum_weighting_model_results:  \n",
    "            expand_fed_part_avg_model_results= {**fed_momentum_weighting_model_results[server_round], \"global_loss\": eval_res[0], \"global_metrics\": eval_res[1]}\n",
    "        else:\n",
    "            expand_fed_part_avg_model_results= {\"global_loss\": eval_res[0], \"global_metrics\": eval_res[1]}\n",
    "        \n",
    "        fed_momentum_weighting_model_results[server_round] = expand_fed_part_avg_model_results\n",
    "        \n",
    "        loss, metrics = eval_res\n",
    "        return loss, metrics\n",
    "\n",
    "    def configure_fit(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, FitIns]]:\n",
    "        \"\"\"Configure the next round of training.\"\"\"\n",
    "        \n",
    "        config = {\"trainable_layers\": self.layer_training_sequence[self.training_sequence_index]}\n",
    "\n",
    "        if self.global_optim_state is not None:\n",
    "            config[\"optimizer_state\"] = self.global_optim_state\n",
    "        \n",
    "        sample_size, min_num_clients = self.num_fit_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "        \n",
    "        print(f\"Training on layer {self.layer_training_sequence}\")\n",
    "        fit_configurations = []\n",
    "        for idx, client in enumerate(clients):\n",
    "            fit_configurations.append((client, FitIns(parameters, config)))\n",
    "\n",
    "        self.training_sequence_index = self.training_sequence_index + 1\n",
    "        \n",
    "        return fit_configurations\n",
    "    \n",
    "\n",
    "    def configure_evaluate(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, EvaluateIns]]:\n",
    "        \"\"\"Configure the next round of evaluation.\"\"\"\n",
    "        if self.fraction_evaluate == 0.0:\n",
    "            return []\n",
    "        config = {}\n",
    "        evaluate_ins = EvaluateIns(parameters, config)\n",
    "\n",
    "        # Sample clients\n",
    "        sample_size, min_num_clients = self.num_evaluation_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "\n",
    "        # Return client/config pairs\n",
    "        return [(client, evaluate_ins) for client in clients]\n",
    "\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "        global global_all_optimizer_states\n",
    "        \"\"\"Aggregate fit results using weighted average.\"\"\"\n",
    "        \n",
    "        total_size = 0\n",
    "        for client, fit_res in results:\n",
    "            total_size += get_parameters_size(fit_res.parameters)\n",
    "        print(f\"total size: {total_size}\")\n",
    "        \n",
    "        weights_results = [\n",
    "            (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)\n",
    "            for _, fit_res in results\n",
    "        ]\n",
    "\n",
    "        metrics = {}\n",
    "        optimizer_states_serialized = [res.metrics.get(\"optimizer_state\", None) for _, res in results]\n",
    "        optimizer_states = [pickle.loads(base64.b64decode(state)) for state in optimizer_states_serialized if state is not None]\n",
    "\n",
    "        # Get cosine similarities\n",
    "        if server_round == 1:\n",
    "            # First round doesn't have a previous state. Use average momentum instead\n",
    "            equal_weights = [1] * len(optimizer_states)\n",
    "            prev_global_optimizer_state = self._aggregate_optimizer_states_weighted(optimizer_states, equal_weights)\n",
    "        else:\n",
    "            prev_global_optimizer_state = pickle.loads(base64.b64decode(self.global_optim_state))\n",
    "        cos_similarities = self._cos_similarity_from_optimizer_states(optimizer_states, prev_global_optimizer_state)\n",
    "\n",
    "        # Aggregate nn params using weighting from momentum\n",
    "        aggregated_weights = self._aggregate_params_weighted(weights_results, cos_similarities)\n",
    "\n",
    "        if self.layer_training_sequence[self.training_sequence_index -1] == -1:\n",
    "            self.latest_parameters = ndarrays_to_parameters(aggregated_weights)\n",
    "        else:\n",
    "            current_model = parameters_to_ndarrays(self.latest_parameters)\n",
    "            current_model[self.layer_training_sequence[self.training_sequence_index -1]] = aggregated_weights[0]\n",
    "            self.latest_parameters = ndarrays_to_parameters(current_model)\n",
    "\n",
    "        # aggregate optimizer state using weighting from momentum\n",
    "        aggregated_optimizer_state = self._aggregate_optimizer_states_weighted(optimizer_states, cos_similarities)\n",
    "        aggregated_optimizer_state_serialized = base64.b64encode(pickle.dumps(aggregated_optimizer_state)).decode('ascii')\n",
    "        metrics[\"optimizer_state\"] = aggregated_optimizer_state_serialized\n",
    "        self.global_optim_state = aggregated_optimizer_state_serialized\n",
    "\n",
    "        # Add sizes to total_size:\n",
    "        sizes = sum([sys.getsizeof(data) for data in optimizer_states_serialized if data is not None])\n",
    "        total_size += sizes\n",
    "\n",
    "        if fed_momentum_weighting_results.get(server_round):\n",
    "            fed_momentum_weighting_results[server_round][\"total_size\"] = total_size\n",
    "        else:\n",
    "            fed_momentum_weighting_results[server_round] = {\"total_size\": total_size}\n",
    "\n",
    "        return self.latest_parameters, metrics\n",
    "\n",
    "\n",
    "    def _momentum_based_aggregate(results: list[tuple[NDArrays, int]], cosine_similarities) -> NDArrays:\n",
    "        \"\"\"Compute weighted average of params.\"\"\"\n",
    "        # Calculate the total number of examples used during training\n",
    "        num_examples_total = sum(num_examples for (_, num_examples) in results)\n",
    "\n",
    "        # Create a list of weights, each multiplied by the related number of examples\n",
    "        weighted_weights = [\n",
    "            [layer * num_examples for layer in weights] for weights, num_examples in results\n",
    "        ]\n",
    "\n",
    "        # Compute average weights of each layer\n",
    "        weights_prime: NDArrays = [\n",
    "            reduce(np.add, layer_updates) / num_examples_total\n",
    "            for layer_updates in zip(*weighted_weights)\n",
    "        ]\n",
    "        return weights_prime\n",
    "\n",
    "    def _aggregate_optimizer_states_weighted(self, optimizer_states, similarities):\n",
    "        # aggregates by comuting mean of 1st and 2nd momentum, and taking the maximum step value. (alternatively, could be changed to reset steps to 0)\n",
    "        # uses direct normalization (weight of similarity divided by sum of similarities)\n",
    "        aggregated_state_dict = copy.deepcopy(optimizer_states[0])\n",
    "        sum_similarities = sum(similarities)\n",
    "\n",
    "        for layer in range(len(optimizer_states[0]['state'])):\n",
    "            weighted_fst_momentum = torch.zeros_like(optimizer_states[0]['state'][layer]['exp_avg'])\n",
    "            weighted_snd_momentum = torch.zeros_like(optimizer_states[0]['state'][layer]['exp_avg_sq'])\n",
    "            \n",
    "            # Accumulate weighted momentums from each client\n",
    "            for i in range(len(optimizer_states)):\n",
    "                weight = similarities[i] / sum_similarities\n",
    "                weighted_fst_momentum += optimizer_states[i]['state'][layer]['exp_avg'] * weight\n",
    "                weighted_snd_momentum += optimizer_states[i]['state'][layer]['exp_avg_sq'] * weight\n",
    "\n",
    "            aggregated_state_dict['state'][layer]['exp_avg'] = weighted_fst_momentum\n",
    "            aggregated_state_dict['state'][layer]['exp_avg_sq'] = weighted_snd_momentum\n",
    "        max_steps = max([optimizer_states[i]['state'][layer]['step'] for i in range(len(optimizer_states))])\n",
    "        aggregated_state_dict['state'][layer]['step'] = max_steps\n",
    "        \n",
    "        return aggregated_state_dict\n",
    "\n",
    "    def _aggregate_params_weighted(self, results: list[tuple[NDArrays, int]], cos_similarities: list[float]) -> NDArrays:\n",
    "        \"\"\"Compute cos similarity weighted average.\"\"\"\n",
    "        # Calculate the total number of examples used during training\n",
    "        assert len(results) == len(cos_similarities)\n",
    "        sum_sim = sum(cos_similarities)\n",
    "\n",
    "        # Create a list of weights, each multiplied by the cos similiarity\n",
    "        weighted_weights = [\n",
    "            [layer * cos_similarities[i] for layer in results[i][0]] for i in range(len(results))\n",
    "        ]\n",
    "\n",
    "        # Compute average weights of each layer\n",
    "        weights_prime: NDArrays = [\n",
    "            reduce(np.add, layer_updates) / sum_sim\n",
    "            for layer_updates in zip(*weighted_weights)\n",
    "        ]\n",
    "        return weights_prime\n",
    "\n",
    "    def _cos_similarity_from_optimizer_states(self, optimizer_states, prev_global_optimizer_state):\n",
    "        # Returns: cosine similarity from each optimizer state to the prev global state\n",
    "        cos = torch.nn.CosineSimilarity(dim=-1)\n",
    "        similarities = []\n",
    "        layers = range(len(optimizer_states[0]['state']))\n",
    "\n",
    "        # get global opt params as vector\n",
    "        fst_momentum_glob = [prev_global_optimizer_state['state'][l]['exp_avg'].view(-1) for l in layers]\n",
    "        snd_momentum_glob = [prev_global_optimizer_state['state'][l]['exp_avg_sq'].view(-1) for l in layers]\n",
    "        glob_state_vec = torch.cat(fst_momentum_glob + snd_momentum_glob)\n",
    "\n",
    "        for client in range(len(optimizer_states)):\n",
    "            # get client opt params as vector\n",
    "            fst_momentum_client = [optimizer_states[client]['state'][l]['exp_avg'].view(-1) for l in layers]\n",
    "            snd_momentum_client = [optimizer_states[client]['state'][l]['exp_avg_sq'].view(-1) for l in layers]\n",
    "            client_state_vec = torch.cat(fst_momentum_client + snd_momentum_client)\n",
    "\n",
    "            sim = cos(client_state_vec, glob_state_vec)\n",
    "            similarities.append(sim.item())\n",
    "\n",
    "        return similarities\n",
    "\n",
    "    def aggregate_evaluate(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, EvaluateRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
    "    ) -> Tuple[Optional[float], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate evaluation losses using weighted average.\"\"\"\n",
    "\n",
    "        if not results:\n",
    "            return None, {}\n",
    "        \n",
    "        total_loss = 0\n",
    "        for _, evaluate_res in results:\n",
    "            total_loss += evaluate_res.loss\n",
    "\n",
    "        if fed_momentum_weighting_results.get(server_round):\n",
    "            fed_momentum_weighting_results[server_round][\"total_loss\"] = total_loss\n",
    "        else:\n",
    "            fed_momentum_weighting_results[server_round] = {\"total_loss\": total_loss}\n",
    "\n",
    "        loss_aggregated = weighted_loss_avg(\n",
    "            [\n",
    "                (evaluate_res.num_examples, evaluate_res.loss)\n",
    "                for _, evaluate_res in results\n",
    "            ]\n",
    "        )\n",
    "        metrics_aggregated = {}\n",
    "        return loss_aggregated, metrics_aggregated\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedAvgPartWithVelocityWeightingFlowerClient(NumPyClient):\n",
    "    def __init__(self, partition_id, net, trainloader, valloader):\n",
    "        self.partition_id = partition_id\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.partition_id}] get_parameters\")\n",
    "        parameters = get_parameters(self.net)\n",
    "        if config[\"trainable_layers\"] == -1:\n",
    "            trained_layers = parameters\n",
    "        else:\n",
    "            trained_layers = [parameters[config[\"trainable_layers\"]]]\n",
    "        \n",
    "        return trained_layers\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        freeze_layers(self.net, config[\"trainable_layers\"])\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters())\n",
    "\n",
    "        if \"optimizer_state\" in config:\n",
    "            print(f\"Got optimizer state in config, setting..\")\n",
    "            serialized_optimizer_state = config[\"optimizer_state\"]\n",
    "            optim_state = pickle.loads(base64.b64decode(serialized_optimizer_state))\n",
    "            self._set_optimizer_state(optim_state)\n",
    "\n",
    "        train(self.net, self.trainloader, epochs=EPOCHS, optimizer=self.optimizer)\n",
    "        print('finished train')\n",
    "\n",
    "        # handle optimizer state (serialize before passing)\n",
    "        optim_state = self._get_optimizer_state()\n",
    "        serialized_optimizer_state = base64.b64encode(pickle.dumps(optim_state)).decode('ascii')\n",
    "        print(f\"After training, got optim state {serialized_optimizer_state[:50]}\")\n",
    "\n",
    "        return self.get_parameters(config), len(self.trainloader), {\"optimizer_state\": serialized_optimizer_state}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "    \n",
    "    def _get_optimizer_state(self):\n",
    "        state_dict = self.optimizer.state_dict()\n",
    "        return state_dict\n",
    "    \n",
    "    def _set_optimizer_state(self, state_dict):\n",
    "        self.optimizer.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def client_fn(context: Context) -> Client:\n",
    "    net = Net().to(DEVICE)\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "    trainloader, valloader, _ = load_datasets(partition_id, num_partitions)\n",
    "    return FedAvgPartWithVelocityWeightingFlowerClient(partition_id, net, trainloader, valloader).to_client()\n",
    "\n",
    "\n",
    "# Create the ClientApp\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(DEVICE)\n",
    "\n",
    "_, _, testloader = load_datasets(0, NUM_PARTITIONS)\n",
    "\n",
    "evaluate_fn = get_evaluate_fn(testloader, net)\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Configure the server for just 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=NUM_OF_ROUNDS)\n",
    "    return ServerAppComponents(\n",
    "        config=config,\n",
    "        strategy=FedPartAvgWithVelocityweighting(\n",
    "            evaluate_fn=evaluate_fn,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "server = ServerApp(server_fn=server_fn)\n",
    "\n",
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_PARTITIONS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot the total size of parameters for each round\n",
    "fed_momentum_weighting_rounds = list(fed_momentum_weighting_results.keys())\n",
    "fed_momentum_weighting_sizes = [fed_momentum_weighting_results[round][\"total_size\"] for round in fed_momentum_weighting_rounds]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fed_momentum_weighting_rounds, fed_momentum_weighting_sizes, marker='o', linestyle='-', color='b', label='FedPartWithVelocityWeighting')\n",
    "# plt.plot(fed_avg_rounds, fed_avg_sizes, marker='o', linestyle='-', color='r', label='FedAvg')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Total Size of Parameters (bytes)')\n",
    "plt.title('Total Size of Parameters for Each Round')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "fed_momentum_weighting_losses = [fed_momentum_weighting_results[round][\"total_loss\"] for round in fed_momentum_weighting_rounds]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fed_momentum_weighting_rounds, fed_momentum_weighting_losses, marker='o', linestyle='-', color='b', label='FedPartWithVelocityWeighting')\n",
    "# plt.plot(fed_avg_rounds, fed_avg_losses, marker='o', linestyle='-', color='r', label='FedAvg')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Total Loss for Each Round')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "fed_momentum_weighting_model_rounds = list(fed_momentum_weighting_model_results.keys())\n",
    "fed_momentum_weighting_accuracies = [fed_momentum_weighting_model_results[round][\"global_metrics\"][\"accuracy\"] for round in fed_momentum_weighting_model_rounds]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fed_momentum_weighting_model_rounds, fed_momentum_weighting_accuracies, marker='o', linestyle='-', color='b', label='FedPartWithVelocityWeighting')\n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_accuracies, marker='o', linestyle='-', color='r', label='FedAvg')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for Each Round')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "fed_momentum_weighting_global_losses = [fed_momentum_weighting_model_results[round][\"global_loss\"] for round in fed_momentum_weighting_model_rounds]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fed_momentum_weighting_model_rounds, fed_momentum_weighting_global_losses, marker='o', linestyle='-', color='b', label='FedPartWithVelocityWeighting')\n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_global_losses, marker='o', linestyle='-', color='r', label='FedAvg')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss for Each Round')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum similarity weighted updates with data heterogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "from flwr_datasets.partitioner import DirichletPartitioner\n",
    "\n",
    "def load_heterogeneous_datasets(partition_id, num_partitions: int):\n",
    "    drichlet_partitioner = DirichletPartitioner(num_partitions=num_partitions, alpha=0.1, partition_by=\"label\")\n",
    "    fds = FederatedDataset(dataset=\"cifar10\", partitioners={\"train\": drichlet_partitioner})\n",
    "    partition = fds.load_partition(partition_id)\n",
    "    # Divide data on each node: 80% train, 20% test\n",
    "    partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
    "    pytorch_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        # Instead of passing transforms to CIFAR10(..., transform=transform)\n",
    "        # we will use this function to dataset.with_transform(apply_transforms)\n",
    "        # The transforms object is exactly the same\n",
    "        batch[\"img\"] = [pytorch_transforms(img) for img in batch[\"img\"]]\n",
    "        return batch\n",
    "\n",
    "    partition_train_test = partition_train_test.with_transform(apply_transforms)\n",
    "    trainloader = DataLoader(partition_train_test[\"train\"], batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(partition_train_test[\"test\"], batch_size=32)\n",
    "    testset = fds.load_split(\"test\").with_transform(apply_transforms)\n",
    "    testloader = DataLoader(testset, batch_size=32)\n",
    "    return trainloader, valloader, testloader\n",
    "\n",
    "\n",
    "def heterogeneous_client_fn(context: Context) -> Client:\n",
    "    net = Net().to(DEVICE)\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "    trainloader, valloader, _ = load_heterogeneous_datasets(partition_id, num_partitions)\n",
    "    return FedAvgPartWithVelocityWeightingFlowerClient(partition_id, net, trainloader, valloader).to_client()\n",
    "\n",
    "\n",
    "# Create the ClientApp\n",
    "client = ClientApp(client_fn=heterogeneous_client_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(DEVICE)\n",
    "\n",
    "_, _, testloader = load_datasets(0, NUM_PARTITIONS)\n",
    "\n",
    "evaluate_fn = get_evaluate_fn(testloader, net)\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Configure the server for just 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=NUM_OF_ROUNDS)\n",
    "    return ServerAppComponents(\n",
    "        config=config,\n",
    "        strategy=FedPartAvgWithVelocityweighting(\n",
    "            evaluate_fn=evaluate_fn,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "server = ServerApp(server_fn=server_fn)\n",
    "\n",
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_PARTITIONS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot the total size of parameters for each round\n",
    "fed_momentum_weighting_rounds = list(fed_momentum_weighting_results.keys())\n",
    "fed_momentum_weighting_sizes = [fed_momentum_weighting_results[round][\"total_size\"] for round in fed_momentum_weighting_rounds]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fed_momentum_weighting_rounds, fed_momentum_weighting_sizes, marker='o', linestyle='-', color='b', label='FedPartWithVelocityWeighting')\n",
    "# plt.plot(fed_avg_rounds, fed_avg_sizes, marker='o', linestyle='-', color='r', label='FedAvg')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Total Size of Parameters (bytes)')\n",
    "plt.title('Total Size of Parameters for Each Round (With Data Heterogeneity)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "fed_momentum_weighting_losses = [fed_momentum_weighting_results[round][\"total_loss\"] for round in fed_momentum_weighting_rounds]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fed_momentum_weighting_rounds, fed_momentum_weighting_losses, marker='o', linestyle='-', color='b', label='FedPartWithVelocityWeighting')\n",
    "# plt.plot(fed_avg_rounds, fed_avg_losses, marker='o', linestyle='-', color='r', label='FedAvg')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Total Loss for Each Round  (With Data Heterogeneity)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "fed_momentum_weighting_model_rounds = list(fed_momentum_weighting_model_results.keys())\n",
    "fed_momentum_weighting_accuracies = [fed_momentum_weighting_model_results[round][\"global_metrics\"][\"accuracy\"] for round in fed_momentum_weighting_model_rounds]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fed_momentum_weighting_model_rounds, fed_momentum_weighting_accuracies, marker='o', linestyle='-', color='b', label='FedPartWithVelocityWeighting')\n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_accuracies, marker='o', linestyle='-', color='r', label='FedAvg')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for Each Round  (With Data Heterogeneity)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "fed_momentum_weighting_global_losses = [fed_momentum_weighting_model_results[round][\"global_loss\"] for round in fed_momentum_weighting_model_rounds]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fed_momentum_weighting_model_rounds, fed_momentum_weighting_global_losses, marker='o', linestyle='-', color='b', label='FedPartWithVelocityWeighting')\n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_global_losses, marker='o', linestyle='-', color='r', label='FedAvg')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss for Each Round  (With Data Heterogeneity)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
