{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q flwr[simulation] flwr-datasets[vision] torch torchvision matplotlib\n",
    "! pip install -U ipywidgets\n",
    "! pip install numpy==1.26.4\n",
    "! pip install urllib3==1.26.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Flower 1.15.1 / PyTorch 2.6.0\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple, Union, Callable\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import DataLoader\n",
    "from flwr.server.strategy import Strategy\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Metrics, Context, Status, GetParametersRes, Parameters, GetParametersIns, MetricsAggregationFn,NDArrays,Scalar\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents \n",
    "from flwr.server.strategy import FedAvg, FedProx\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    "    ParametersRecord,\n",
    "    array_from_numpy\n",
    ")\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.strategy.aggregate import aggregate, weighted_loss_avg\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def load_datasets(partition_id, num_partitions: int):\n",
    "    fds = FederatedDataset(dataset=\"cifar10\", partitioners={\"train\": num_partitions})\n",
    "    partition = fds.load_partition(partition_id)\n",
    "    # Divide data on each node: 80% train, 20% test\n",
    "    partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
    "    pytorch_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        \n",
    "        batch[\"img\"] = [pytorch_transforms(img) for img in batch[\"img\"]]\n",
    "        return batch\n",
    "\n",
    "    partition_train_test = partition_train_test.with_transform(apply_transforms)\n",
    "    trainloader = DataLoader(partition_train_test[\"train\"], batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(partition_train_test[\"test\"], batch_size=32)\n",
    "    testset = fds.load_split(\"test\").with_transform(apply_transforms)\n",
    "    testloader = DataLoader(testset, batch_size=32)\n",
    "    return trainloader, valloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(256*4*4, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 256*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MoonNet(nn.Module):\n",
    "    \"\"\"Returns both the representation (penultimate layer output) and classification\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super(MoonNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(256*4*4, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 256*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        representation = x.clone()\n",
    "        classification = self.fc3(x)\n",
    "        return representation, classification\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "def set_parameters(net, parameters, trainable_layers=-1):\n",
    "    \"\"\"Set model parameters from a list of NumPy arrays.\"\"\"\n",
    "    current_state = OrderedDict(net.state_dict())\n",
    "    \n",
    "    if trainable_layers == -1:\n",
    "        # Update all parameters\n",
    "        params_dict = zip(current_state.keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "        net.load_state_dict(state_dict, strict=True)\n",
    "    else:\n",
    "        # Only update the specified layer's parameters\n",
    "        # Convert current state to numpy arrays\n",
    "        numpy_state = [param.cpu().numpy() for param in current_state.values()]\n",
    "        \n",
    "        # Update the specific indices with new parameters\n",
    "        numpy_state[trainable_layers*2] = parameters[0]\n",
    "        numpy_state[trainable_layers*2 + 1] = parameters[1]\n",
    "        \n",
    "        # Convert back to torch and update state dict\n",
    "        for idx, key in enumerate(current_state.keys()):\n",
    "            current_state[key] = torch.from_numpy(numpy_state[idx])\n",
    "        \n",
    "        net.load_state_dict(current_state, strict=True)\n",
    "\n",
    "# def set_parameters(net, parameters: List[np.ndarray]):\n",
    "#     params_dict = zip(net.state_dict().keys(), parameters)\n",
    "#     state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "#     net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "def train(net, trainloader, epochs: int):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for batch in trainloader:\n",
    "            images, labels = batch[\"img\"], batch[\"label\"]\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "        \n",
    "def proxima_train(net, trainloader, epochs: int, proximal_mu:float, global_params:List[torch.Tensor]):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for batch in trainloader:\n",
    "            images, labels = batch[\"img\"], batch[\"label\"]\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "\n",
    "            proximal_term = 0.0\n",
    "            for local_weights, global_weights in zip(net.parameters(), global_params):\n",
    "                proximal_term += (local_weights - global_weights).norm(2)\n",
    "            loss = criterion(net(images), labels) + (proximal_mu / 2) * proximal_term\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "\n",
    "\n",
    "def train_moon(net,train_loader, global_net,previous_net, epochs, mu, temperature):\n",
    "    \"\"\"Training function for MOON.\"\"\"\n",
    "    print(f\"Started training moon\")\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    previous_net.eval()\n",
    "    global_net.eval()\n",
    "\n",
    "    cnt = 0\n",
    "    cos = torch.nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss_collector = []\n",
    "        epoch_loss1_collector = []\n",
    "        epoch_loss2_collector = []\n",
    "        for batch in train_loader:\n",
    "            x, target = batch[\"img\"], batch[\"label\"]\n",
    "            x, target = x.to(DEVICE), target.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # pro1 is the representation by the current model (Line 14 of Algorithm 1)\n",
    "            pro1, out = net(x)\n",
    "            # pro2 is the representation by the global model (Line 15 of Algorithm 1)\n",
    "            # pro3 is the representation by the previous model (Line 16 of Algorithm 1)\n",
    "            with torch.no_grad():\n",
    "                pro2, _ = global_net(x)\n",
    "                pro3, _ = previous_net(x)\n",
    "\n",
    "            # posi is the positive pair\n",
    "            posi = cos(pro1, pro2)\n",
    "            logits = posi.reshape(-1, 1)\n",
    "\n",
    "            # nega is the negative pair\n",
    "            nega = cos(pro1, pro3)\n",
    "            logits = torch.cat((logits, nega.reshape(-1, 1)), dim=1)\n",
    "\n",
    "            previous_net.to(\"cpu\")\n",
    "            logits /= temperature\n",
    "            labels = torch.zeros(x.size(0)).to(DEVICE).long()\n",
    "\n",
    "            # compute the model-contrastive loss (Line 17 of Algorithm 1)\n",
    "            loss2 = mu * criterion(logits, labels)\n",
    "\n",
    "            # compute the cross-entropy loss (Line 13 of Algorithm 1)\n",
    "            loss1 = criterion(out, target)\n",
    "\n",
    "            # compute the loss (Line 18 of Algorithm 1)\n",
    "            loss = loss1 + loss2\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cnt += 1\n",
    "            epoch_loss_collector.append(loss.item())\n",
    "            epoch_loss1_collector.append(loss1.item())\n",
    "            epoch_loss2_collector.append(loss2.item())\n",
    "\n",
    "        epoch_loss = sum(epoch_loss_collector) / len(epoch_loss_collector)\n",
    "        epoch_loss1 = sum(epoch_loss1_collector) / len(epoch_loss1_collector)\n",
    "        epoch_loss2 = sum(epoch_loss2_collector) / len(epoch_loss2_collector)\n",
    "        print(\n",
    "            \"Epoch: %d Loss: %f Loss1: %f Loss2: %f\"\n",
    "            % (epoch, epoch_loss, epoch_loss1, epoch_loss2)\n",
    "        )\n",
    "\n",
    "\n",
    "def test_moon(net, testloader):\n",
    "    \"\"\"\n",
    "    Evaluate the network on the entire test set.\n",
    "    Same as the regular test, but using the MoonNet \n",
    "    (where the output is a tuple of (representation, classification) )\n",
    "    \"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images, labels = batch[\"img\"], batch[\"label\"]\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            _, outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images, labels = batch[\"img\"], batch[\"label\"]\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy\n",
    "\n",
    "# def freeze_layers(model: torch.nn.Module, trainable_layers: int) -> None:\n",
    "#         \"\"\"Freeze specified layers of the model.\"\"\"\n",
    "#         for idx, (name, param) in enumerate(model.named_parameters()):\n",
    "#             if idx == trainable_layers or trainable_layers == -1:\n",
    "#                 param.requires_grad = True\n",
    "#             else:\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "def freeze_layers(model: torch.nn.Module, trainable_layers: int) -> None:\n",
    "        \"\"\"Freeze specified layers of the model.\"\"\"\n",
    "        trainable_layers_set = []\n",
    "        if trainable_layers == -1:\n",
    "            trainable_layers_set = [-1]\n",
    "        else:\n",
    "            trainable_layers_set = [trainable_layers *2, trainable_layers *2 +1]\n",
    "\n",
    "        for idx, (name, param) in enumerate(model.named_parameters()):\n",
    "            \n",
    "            if idx in trainable_layers_set or trainable_layers_set[0] == -1:\n",
    "                param.requires_grad = True\n",
    "                print(f\"layer index is {idx} and name{name} is trainabe\")\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                print(f\"layer index is {idx} and name{name} is frozen\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rounds: 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NETWORK_LEN = len(Net().state_dict().keys()) //2 \n",
    "EPOCHS = 8\n",
    "NUM_PARTITIONS = 6\n",
    "NUM_OF_CYCLES  = 1\n",
    "NUM_OF_FULL_UPDATES_BETWEEN_CYCLES = 2\n",
    "NUM_OF_ROUNDS = (NUM_OF_CYCLES * NUM_OF_FULL_UPDATES_BETWEEN_CYCLES) + (NUM_OF_CYCLES * NETWORK_LEN *2)\n",
    "print(f\"Number of rounds: {NUM_OF_ROUNDS}\")\n",
    "backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.common import NDArrays, Scalar\n",
    "import sys\n",
    "\n",
    "# More robust evaluate function:\n",
    "def get_evaluate_fn(\n",
    "    testloader: DataLoader,\n",
    "    net: torch.nn.Module,\n",
    ") -> Callable[[int, NDArrays, Dict[str, Scalar]], Optional[Tuple[float, Dict[str, Scalar]]]]:\n",
    "    \"\"\"Return an evaluation function for server-side evaluation.\"\"\"\n",
    "    \n",
    "    # used to check if they're changing\n",
    "    previous_params = None\n",
    "    \n",
    "    def evaluate(\n",
    "        server_round: int, parameters: NDArrays, config: Dict[str, Scalar]\n",
    "    ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "        \"\"\"Use the entire test set for evaluation.\"\"\"\n",
    "        nonlocal previous_params\n",
    "        \n",
    "        print(f\"\\n==== Server-side evaluation for round {server_round} ====\")\n",
    "        \n",
    "        # Check if parameters changed from previous round\n",
    "        if previous_params is not None:\n",
    "            param_change = False\n",
    "            for i, (prev, curr) in enumerate(zip(previous_params, parameters)):\n",
    "                diff = np.abs(prev - curr).mean()\n",
    "                if diff > 1e-6:\n",
    "                    param_change = True\n",
    "                    print(f\"  Parameter {i}: Changed by {diff:.6f}\")\n",
    "            \n",
    "            if not param_change:\n",
    "                print(\"  WARNING: Parameters haven't changed from previous round!\")\n",
    "        \n",
    "        previous_params = [p.copy() for p in parameters]\n",
    "        net_copy = copy.deepcopy(net)\n",
    "\n",
    "        # Update model with the latest parameters\n",
    "        params_dict = zip(net_copy.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v, device=DEVICE) for k, v in params_dict})\n",
    "        \n",
    "        # Check if state dict keys match model keys\n",
    "        model_keys = set(net_copy.state_dict().keys())\n",
    "        params_keys = set(state_dict.keys())\n",
    "        if model_keys != params_keys:\n",
    "            print(f\"  WARNING: Key mismatch between model and parameters!\")\n",
    "            print(f\"  Missing in params: {model_keys - params_keys}\")\n",
    "            print(f\"  Extra in params: {params_keys - model_keys}\")\n",
    "        \n",
    "        net_copy.load_state_dict(state_dict, strict=True)\n",
    "        net_copy.to(DEVICE)\n",
    "        net_copy.eval()\n",
    "        \n",
    "        # Test the model\n",
    "        loss, accuracy = test(net_copy, testloader)\n",
    "        print(f\"  Evaluation results - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Return loss and metrics\n",
    "        return loss, {\"accuracy\": accuracy}\n",
    "    \n",
    "    return evaluate\n",
    "\n",
    "\n",
    "\n",
    "def get_evaluate_fn_moon(\n",
    "    testloader: DataLoader,\n",
    "    net: torch.nn.Module,\n",
    ") -> Callable[[int, NDArrays, Dict[str, Scalar]], Optional[Tuple[float, Dict[str, Scalar]]]]:\n",
    "    \"\"\"Return an evaluation function for server-side evaluation.\"\"\"\n",
    "\n",
    "    def evaluate(\n",
    "        server_round: int, parameters: NDArrays, config: Dict[str, Scalar]\n",
    "    ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "        \"\"\"Use the entire test set for evaluation.\"\"\"\n",
    "        \n",
    "        # Copy model parameters to avoid modifying the original\n",
    "        net_copy = copy.deepcopy(net)\n",
    "        \n",
    "        # Update model with the latest parameters\n",
    "        params_dict = zip(net_copy.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "        net_copy.load_state_dict(state_dict, strict=True)\n",
    "        \n",
    "        net_copy.to(DEVICE)\n",
    "        net_copy.eval()\n",
    "\n",
    "        # Test the model\n",
    "        loss, accuracy = test_moon(net_copy, testloader)\n",
    "        \n",
    "        # Return loss and metrics\n",
    "        return loss, {\"accuracy\": accuracy}\n",
    "\n",
    "    return evaluate\n",
    "\n",
    "\n",
    "def get_parameters_size(params: Parameters) -> int:\n",
    "    size = sys.getsizeof(params)  # Base size of the dataclass instance\n",
    "    size += sys.getsizeof(params.tensor_type)  # Size of the string\n",
    "    size += sys.getsizeof(params.tensors)  # Size of the list container\n",
    "    size += sum(sys.getsizeof(tensor) for tensor in params.tensors)  # Size of each bytes object\n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.strategy.aggregate import aggregate, weighted_loss_avg\n",
    "\n",
    "\n",
    "fed_avg_result = {}\n",
    "fed_avg_model_results = {}\n",
    "\n",
    "class ModifiedFedAvg(Strategy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fraction_fit: float = 1.0,\n",
    "        fraction_evaluate: float = 1.0,\n",
    "        min_fit_clients: int = 2,\n",
    "        min_evaluate_clients: int = 2,\n",
    "        min_available_clients: int = 2,\n",
    "        evaluate_fn: Optional[\n",
    "            Callable[\n",
    "                [int, NDArrays, dict[str, Scalar]],\n",
    "                Optional[tuple[float, dict[str, Scalar]]],\n",
    "            ]\n",
    "        ] = None,\n",
    "        on_fit_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,\n",
    "        on_evaluate_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,\n",
    "        accept_failures: bool = True,\n",
    "        initial_parameters: Optional[Parameters] = None,\n",
    "        fit_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
    "        evaluate_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
    "        inplace: bool = True,\n",
    "        layer_update_strategy: str = \"sequential\",\n",
    "        \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.fraction_fit = fraction_fit\n",
    "        self.fraction_evaluate = fraction_evaluate\n",
    "        self.min_fit_clients = min_fit_clients\n",
    "        self.min_evaluate_clients = min_evaluate_clients\n",
    "        self.min_available_clients = min_available_clients\n",
    "        self.evaluate_fn = evaluate_fn\n",
    "        self.on_fit_config_fn = on_fit_config_fn\n",
    "        self.on_evaluate_config_fn = on_evaluate_config_fn\n",
    "        self.accept_failures = accept_failures\n",
    "        self.initial_parameters = initial_parameters\n",
    "        self.fit_metrics_aggregation_fn = fit_metrics_aggregation_fn\n",
    "        self.evaluate_metrics_aggregation_fn = evaluate_metrics_aggregation_fn\n",
    "        self.inplace = inplace\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"FedPartAvg\"\n",
    "    \n",
    "\n",
    "    def num_fit_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Return sample size and required number of clients.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_fit)\n",
    "        return max(num_clients, self.min_fit_clients), self.min_available_clients\n",
    "\n",
    "    def num_evaluation_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Use a fraction of available clients for evaluation.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_evaluate)\n",
    "        return max(num_clients, self.min_evaluate_clients), self.min_available_clients\n",
    "    \n",
    "   \n",
    "    # def initialize_parameters(\n",
    "    #     self, client_manager: ClientManager\n",
    "    # ) -> Optional[Parameters]:\n",
    "    #     \"\"\"Initialize global model parameters.\"\"\"\n",
    "    #     net = Net()\n",
    "    #     ndarrays = get_parameters(net)\n",
    "    #     return ndarrays_to_parameters(ndarrays)\n",
    "    def initialize_parameters(\n",
    "        self, client_manager: ClientManager) -> Optional[Parameters]:\n",
    "        \"\"\"Initialize global model parameters.\"\"\"\n",
    "        print(\"Initializing global model parameters\")\n",
    "        net = Net().to(DEVICE)  # Make sure to initialize on the correct device\n",
    "        ndarrays = get_parameters(net)\n",
    "        \n",
    "        # Debug: print summary of parameters to verify they're not all zeros or random\n",
    "        for i, param in enumerate(ndarrays):\n",
    "            print(f\"Global init param {i}: shape={param.shape}, mean={param.mean():.6f}, std={param.std():.6f}\")\n",
    "        \n",
    "        return ndarrays_to_parameters(ndarrays) \n",
    "\n",
    "    def evaluate(\n",
    "        self, server_round: int, parameters: Parameters\n",
    "    ) -> Optional[tuple[float, dict[str, Scalar]]]:\n",
    "        \"\"\"Evaluate model parameters using an evaluation function.\"\"\"\n",
    "        if self.evaluate_fn is None:\n",
    "            # No evaluation function provided\n",
    "            return None\n",
    "        parameters_ndarrays = parameters_to_ndarrays(parameters)\n",
    "        eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})\n",
    "        if eval_res is None:\n",
    "            return None\n",
    "        loss, metrics = eval_res\n",
    "\n",
    "        if server_round in fed_avg_model_results:\n",
    "            expand_fed_avg_result= {**fed_avg_model_results[server_round], \"global_loss\": loss, \"global_metrics\": metrics}\n",
    "        else:\n",
    "            expand_fed_avg_result= {\"global_loss\": loss, \"global_metrics\": metrics}\n",
    "\n",
    "        fed_avg_model_results[server_round] = expand_fed_avg_result\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "    def configure_fit(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, FitIns]]:\n",
    "        \"\"\"Configure the next round of training.\"\"\"\n",
    "        \n",
    "        config = {}\n",
    "        \n",
    "        sample_size, min_num_clients = self.num_fit_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "        \n",
    "        fit_configurations = []\n",
    "        for idx, client in enumerate(clients):\n",
    "            fit_configurations.append((client, FitIns(parameters, config)))\n",
    "\n",
    "        \n",
    "        return fit_configurations\n",
    "    \n",
    "\n",
    "    def configure_evaluate(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, EvaluateIns]]:\n",
    "        \"\"\"Configure the next round of evaluation.\"\"\"\n",
    "        if self.fraction_evaluate == 0.0:\n",
    "            return []\n",
    "        config = {}\n",
    "        evaluate_ins = EvaluateIns(parameters, config)\n",
    "\n",
    "        # Sample clients\n",
    "        sample_size, min_num_clients = self.num_evaluation_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "\n",
    "        # Return client/config pairs\n",
    "        return [(client, evaluate_ins) for client in clients]\n",
    "\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate fit results using weighted average.\"\"\"\n",
    "\n",
    "        # get size of parameters in bytes\n",
    "        total_size = 0\n",
    "        for client, fit_res in results:\n",
    "            total_size += get_parameters_size(fit_res.parameters) *2\n",
    "        \n",
    "\n",
    "        if server_round in fed_avg_result:\n",
    "            expand_fed_avg_result= {**fed_avg_result[server_round], \"total_size\": total_size}\n",
    "        else:\n",
    "            expand_fed_avg_result= {\"total_size\": total_size}\n",
    "\n",
    "        fed_avg_result[server_round] = expand_fed_avg_result\n",
    "\n",
    "\n",
    "        weights_results = [\n",
    "            (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)\n",
    "            for _, fit_res in results\n",
    "        ]\n",
    "        parameters_aggregated = ndarrays_to_parameters(aggregate(weights_results))\n",
    "        metrics_aggregated = {}\n",
    "        return parameters_aggregated, metrics_aggregated\n",
    "\n",
    "\n",
    "    def aggregate_evaluate(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, EvaluateRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
    "    ) -> Tuple[Optional[float], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate evaluation losses using weighted average.\"\"\"\n",
    "\n",
    "        if not results:\n",
    "            return None, {}\n",
    "\n",
    "        total_loss = 0\n",
    "        for _, evaluate_res in results:\n",
    "            total_loss += evaluate_res.loss \n",
    "\n",
    "\n",
    "        if server_round in fed_avg_result:\n",
    "            expand_fed_avg_result= {**fed_avg_result[server_round], \"total_loss\": total_loss}\n",
    "        else:\n",
    "            expand_fed_avg_result= {\"total_loss\": total_loss}\n",
    "\n",
    "        fed_avg_result[server_round] = expand_fed_avg_result\n",
    "\n",
    "        loss_aggregated = weighted_loss_avg(\n",
    "            [\n",
    "                (evaluate_res.num_examples, evaluate_res.loss)\n",
    "                for _, evaluate_res in results\n",
    "            ]\n",
    "        )\n",
    "        metrics_aggregated = {}\n",
    "        return loss_aggregated, metrics_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalFlowerClient(NumPyClient):\n",
    "    def __init__(self, partition_id, net, trainloader, valloader):\n",
    "        self.partition_id = partition_id\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.partition_id}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, epochs=EPOCHS)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "\n",
    "def client_fn(context: Context) -> Client:\n",
    "    net = Net().to(DEVICE)\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "    trainloader, valloader, _ = load_datasets(partition_id, num_partitions)\n",
    "    return NormalFlowerClient(partition_id, net, trainloader, valloader).to_client()\n",
    "\n",
    "\n",
    "# Create the ClientApp\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(10952) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(10953) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=20, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "python(10954) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(10955) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(10956) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing global model parameters\n",
      "Global init param 0: shape=(32, 3, 3, 3), mean=-0.002421, std=0.108616\n",
      "Global init param 1: shape=(32,), mean=0.050804, std=0.104413\n",
      "Global init param 2: shape=(64, 32, 3, 3), mean=0.000256, std=0.033960\n",
      "Global init param 3: shape=(64,), mean=-0.001014, std=0.031368\n",
      "Global init param 4: shape=(128, 64, 3, 3), mean=-0.000029, std=0.024125\n",
      "Global init param 5: shape=(128,), mean=-0.002976, std=0.025366\n",
      "Global init param 6: shape=(128, 128, 3, 3), mean=0.000052, std=0.016992\n",
      "Global init param 7: shape=(128,), mean=0.001150, std=0.017278\n",
      "Global init param 8: shape=(256, 128, 3, 3), mean=0.000006, std=0.017024\n",
      "Global init param 9: shape=(256,), mean=0.001124, std=0.017165\n",
      "Global init param 10: shape=(256, 256, 3, 3), mean=0.000034, std=0.012019\n",
      "Global init param 11: shape=(256,), mean=-0.000360, std=0.011914\n",
      "Global init param 12: shape=(1024, 4096), mean=0.000005, std=0.009019\n",
      "Global init param 13: shape=(1024,), mean=0.000140, std=0.008901\n",
      "Global init param 14: shape=(512, 1024), mean=-0.000006, std=0.018050\n",
      "Global init param 15: shape=(512,), mean=-0.000324, std=0.018125\n",
      "Global init param 16: shape=(10, 512), mean=-0.000100, std=0.025607\n",
      "Global init param 17: shape=(10,), mean=0.012431, std=0.021466\n",
      "\n",
      "==== Server-side evaluation for round 0 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(10957) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(10958) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(10959) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(10960) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(10961) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 0.07208211109638214, {'accuracy': 0.1}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 6 clients (out of 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluation results - Loss: 0.0721, Accuracy: 0.1000\n",
      "\u001b[36m(ClientAppActor pid=10977)\u001b[0m [Client 0] fit, config: {}\n",
      "\u001b[36m(ClientAppActor pid=10977)\u001b[0m Epoch 1: train loss 0.06606657058000565, accuracy 0.17159142042897854\n",
      "\u001b[36m(ClientAppActor pid=10974)\u001b[0m [Client 5] fit, config: {}\u001b[32m [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=10977)\u001b[0m Epoch 2: train loss 0.057929206639528275, accuracy 0.2777861106944653\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=10977)\u001b[0m Epoch 3: train loss 0.053263384848833084, accuracy 0.34588270586470676\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=10977)\u001b[0m Epoch 4: train loss 0.04899737611413002, accuracy 0.40977951102444876\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=10977)\u001b[0m Epoch 5: train loss 0.04532875120639801, accuracy 0.45642717864106797\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=10979)\u001b[0m Epoch 6: train loss 0.038419172167778015, accuracy 0.5546054605460546\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=10977)\u001b[0m Epoch 7: train loss 0.038177113980054855, accuracy 0.551822408879556\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=10978)\u001b[0m Epoch 7: train loss 0.03740597888827324, accuracy 0.5567056705670567\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=10977)\u001b[0m Epoch 8: train loss 0.03399508073925972, accuracy 0.5974201289935503\n",
      "\u001b[36m(ClientAppActor pid=10979)\u001b[0m Epoch 8: train loss 0.0292363204061985, accuracy 0.656915691569157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 6 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Server-side evaluation for round 1 ====\n",
      "  Parameter 0: Changed by 0.023231\n",
      "  Parameter 1: Changed by 0.042306\n",
      "  Parameter 2: Changed by 0.019566\n",
      "  Parameter 3: Changed by 0.015253\n",
      "  Parameter 4: Changed by 0.013653\n",
      "  Parameter 5: Changed by 0.021924\n",
      "  Parameter 6: Changed by 0.012905\n",
      "  Parameter 7: Changed by 0.019934\n",
      "  Parameter 8: Changed by 0.011480\n",
      "  Parameter 9: Changed by 0.018992\n",
      "  Parameter 10: Changed by 0.008972\n",
      "  Parameter 11: Changed by 0.017740\n",
      "  Parameter 12: Changed by 0.006005\n",
      "  Parameter 13: Changed by 0.012385\n",
      "  Parameter 14: Changed by 0.006454\n",
      "  Parameter 15: Changed by 0.017873\n",
      "  Parameter 16: Changed by 0.014457\n",
      "  Parameter 17: Changed by 0.026770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      fit progress: (1, 0.07246455738544465, {'accuracy': 0.1045}, 555.2189027080003)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 6 clients (out of 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluation results - Loss: 0.0725, Accuracy: 0.1045\n",
      "\u001b[36m(ClientAppActor pid=10976)\u001b[0m [Client 0] evaluate, config: {}\n",
      "\u001b[36m(ClientAppActor pid=10978)\u001b[0m Epoch 8: train loss 0.03321531042456627, accuracy 0.6068106810681068\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 6 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 6 clients (out of 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=10978)\u001b[0m [Client 5] fit, config: {}\n",
      "\u001b[36m(ClientAppActor pid=10979)\u001b[0m [Client 2] evaluate, config: {}\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=10978)\u001b[0m Epoch 1: train loss 0.0594174824655056, accuracy 0.24932493249324933\n",
      "\u001b[36m(ClientAppActor pid=10977)\u001b[0m [Client 1] fit, config: {}\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=10979)\u001b[0m Epoch 2: train loss 0.05306341126561165, accuracy 0.3431828408579571\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=10975)\u001b[0m Epoch 3: train loss 0.04715561866760254, accuracy 0.4237923792379238\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=10975)\u001b[0m Epoch 4: train loss 0.04178646206855774, accuracy 0.49894989498949893\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=10975)\u001b[0m Epoch 5: train loss 0.03755224868655205, accuracy 0.5528052805280528\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=10977)\u001b[0m Epoch 5: train loss 0.03957195207476616, accuracy 0.5306734663266837\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "net = Net().to(DEVICE)\n",
    "\n",
    "_, _, testloader = load_datasets(0, NUM_PARTITIONS)\n",
    "\n",
    "evaluate_fn = get_evaluate_fn(testloader, net)\n",
    "\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Configure the server for just 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=NUM_OF_ROUNDS)\n",
    "    return ServerAppComponents(\n",
    "        config=config,\n",
    "        strategy=ModifiedFedAvg(\n",
    "            evaluate_fn=evaluate_fn\n",
    "        ),\n",
    "    )\n",
    "\n",
    "server = ServerApp(server_fn=server_fn)\n",
    "\n",
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_PARTITIONS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "with open(f'results/fed_avg_results.p', 'wb') as file:\n",
    "    pickle.dump(fed_avg_result, file)\n",
    "\n",
    "with open(f'results/fed_avg_model_results.p', 'wb') as file:\n",
    "    pickle.dump(fed_avg_model_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# fed_avg_rounds = list(fed_avg_result.keys())\n",
    "# fed_avg_sizes = [fed_avg_result[round][\"total_size\"] for round in fed_avg_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_avg_rounds, fed_avg_sizes, marker='o', linestyle='-', color='b')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Total Size of Parameters (bytes)')\n",
    "# plt.title('Total Size of Parameters for Each Round')\n",
    "# plt.grid(True)\n",
    "\n",
    "# fed_avg_losses = [fed_avg_result[round][\"total_loss\"] for round in fed_avg_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_avg_rounds, fed_avg_losses, marker='o', linestyle='-', color='b')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Total Loss')\n",
    "# plt.title('Total Loss for Each Round')\n",
    "# plt.grid(True)\n",
    "\n",
    "# fed_avg_model_rounds = list(fed_avg_model_results.keys())\n",
    "\n",
    "# fed_avg_accuracies = [fed_avg_model_results[round][\"global_metrics\"][\"accuracy\"] for round in fed_avg_model_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_accuracies, marker='o', linestyle='-', color='b')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Accuracy for Each Round')\n",
    "# plt.grid(True)\n",
    "\n",
    "# fed_avg_global_losses = [fed_avg_model_results[round][\"global_loss\"] for round in fed_avg_model_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_global_losses, marker='o', linestyle='-', color='b')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Loss for Each Round')\n",
    "# plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedProx experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedProxFlowerClient(NumPyClient):\n",
    "    def __init__(self, partition_id, net, trainloader, valloader):\n",
    "        self.partition_id = partition_id\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.partition_id}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        global_params = copy.deepcopy(self.net).parameters()\n",
    "        proxima_train(self.net, self.trainloader, EPOCHS, config[\"proximal_mu\"], global_params)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "def client_fn(context: Context) -> Client:\n",
    "    net = Net().to(DEVICE)\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "    trainloader, valloader, _ = load_datasets(partition_id, num_partitions)\n",
    "    return FedProxFlowerClient(partition_id, net, trainloader, valloader).to_client()\n",
    "\n",
    "\n",
    "# Create the ClientApp\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_prox_result = {}\n",
    "\n",
    "fed_prox_model_results = {}\n",
    "\n",
    "class ModifiedFedProx(ModifiedFedAvg):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        fraction_fit: float = 1.0,\n",
    "        fraction_evaluate: float = 1.0,\n",
    "        min_fit_clients: int = 2,\n",
    "        min_evaluate_clients: int = 2,\n",
    "        min_available_clients: int = 2,\n",
    "        evaluate_fn: Optional[\n",
    "            Callable[\n",
    "                [int, NDArrays, dict[str, Scalar]],\n",
    "                Optional[tuple[float, dict[str, Scalar]]],\n",
    "            ]\n",
    "        ] = None,\n",
    "        on_fit_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,\n",
    "        on_evaluate_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,\n",
    "        accept_failures: bool = True,\n",
    "        initial_parameters: Optional[Parameters] = None,\n",
    "        fit_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
    "        evaluate_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
    "        proximal_mu: float,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            fraction_fit=fraction_fit,\n",
    "            fraction_evaluate=fraction_evaluate,\n",
    "            min_fit_clients=min_fit_clients,\n",
    "            min_evaluate_clients=min_evaluate_clients,\n",
    "            min_available_clients=min_available_clients,\n",
    "            evaluate_fn=evaluate_fn,\n",
    "            on_fit_config_fn=on_fit_config_fn,\n",
    "            on_evaluate_config_fn=on_evaluate_config_fn,\n",
    "            accept_failures=accept_failures,\n",
    "            initial_parameters=initial_parameters,\n",
    "            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n",
    "            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n",
    "        )\n",
    "        self.proximal_mu = proximal_mu\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"ModifiedFedProx\"\n",
    "    \n",
    "\n",
    "    def configure_fit(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> list[tuple[ClientProxy, FitIns]]:\n",
    "        \"\"\"Configure the next round of training.\n",
    "\n",
    "        Sends the proximal factor mu to the clients\n",
    "        \"\"\"\n",
    "        # Get the standard client/config pairs from the FedAvg super-class\n",
    "        client_config_pairs = super().configure_fit(\n",
    "            server_round, parameters, client_manager\n",
    "        )\n",
    "\n",
    "        # Return client/config pairs with the proximal factor mu added\n",
    "        return [\n",
    "            (\n",
    "                client,\n",
    "                FitIns(\n",
    "                    fit_ins.parameters,\n",
    "                    {**fit_ins.config, \"proximal_mu\": self.proximal_mu},\n",
    "                ),\n",
    "            )\n",
    "            for client, fit_ins in client_config_pairs\n",
    "        ]\n",
    "    \n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate fit results using weighted average.\"\"\"\n",
    "        \n",
    "        total_size = 0\n",
    "        for client, fit_res in results:\n",
    "            total_size += get_parameters_size(fit_res.parameters) *2\n",
    "        print(f\"total size: {total_size}\")\n",
    "        \n",
    "        if fed_prox_result.get(server_round):\n",
    "            fed_prox_result[server_round][\"total_size\"] = total_size\n",
    "        else:\n",
    "            fed_prox_result[server_round] = {\"total_size\": total_size}\n",
    "        \n",
    "\n",
    "        weights_results = [\n",
    "            (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)\n",
    "            for _, fit_res in results\n",
    "        ]\n",
    "\n",
    "        parameters_aggregated = ndarrays_to_parameters(aggregate(weights_results))\n",
    "        metrics_aggregated = {}\n",
    "        return parameters_aggregated, metrics_aggregated\n",
    "\n",
    "\n",
    "    def aggregate_evaluate(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, EvaluateRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
    "    ) -> Tuple[Optional[float], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate evaluation losses using weighted average.\"\"\"\n",
    "\n",
    "        if not results:\n",
    "            return None, {}\n",
    "        \n",
    "        total_loss = 0\n",
    "        for _, evaluate_res in results:\n",
    "            total_loss += evaluate_res.loss\n",
    "\n",
    "        if fed_prox_result.get(server_round):\n",
    "            fed_prox_result[server_round][\"total_loss\"] = total_loss\n",
    "        else:\n",
    "            fed_prox_result[server_round] = {\"total_loss\": total_loss}\n",
    "\n",
    "        loss_aggregated = weighted_loss_avg(\n",
    "            [\n",
    "                (evaluate_res.num_examples, evaluate_res.loss)\n",
    "                for _, evaluate_res in results\n",
    "            ]\n",
    "        )\n",
    "        metrics_aggregated = {}\n",
    "        return loss_aggregated, metrics_aggregated\n",
    "    \n",
    "\n",
    "    def evaluate(\n",
    "        self, server_round: int, parameters: Parameters\n",
    "    ) -> Optional[tuple[float, dict[str, Scalar]]]:\n",
    "        \"\"\"Evaluate model parameters using an evaluation function.\"\"\"\n",
    "        if self.evaluate_fn is None:\n",
    "            # No evaluation function provided\n",
    "            return None\n",
    "        parameters_ndarrays = parameters_to_ndarrays(parameters)\n",
    "        eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})\n",
    "        if eval_res is None:\n",
    "            return None\n",
    "        \n",
    "        if server_round in fed_prox_model_results:  \n",
    "            expand_fed_prox_model_results= {**fed_prox_model_results[server_round], \"global_loss\": eval_res[0], \"global_metrics\": eval_res[1]}\n",
    "        else:\n",
    "            expand_fed_prox_model_results= {\"global_loss\": eval_res[0], \"global_metrics\": eval_res[1]}\n",
    "        \n",
    "        fed_prox_model_results[server_round] = expand_fed_prox_model_results\n",
    "        \n",
    "        loss, metrics = eval_res\n",
    "        return loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=20, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing global model parameters\n",
      "Global init param 0: shape=(32, 3, 3, 3), mean=0.001741, std=0.112499\n",
      "Global init param 1: shape=(32,), mean=0.011245, std=0.122499\n",
      "Global init param 2: shape=(64, 32, 3, 3), mean=-0.000072, std=0.033968\n",
      "Global init param 3: shape=(64,), mean=0.002386, std=0.033743\n",
      "Global init param 4: shape=(128, 64, 3, 3), mean=0.000095, std=0.024095\n",
      "Global init param 5: shape=(128,), mean=0.002493, std=0.024324\n",
      "Global init param 6: shape=(128, 128, 3, 3), mean=-0.000037, std=0.017028\n",
      "Global init param 7: shape=(128,), mean=-0.000770, std=0.016107\n",
      "Global init param 8: shape=(256, 128, 3, 3), mean=-0.000027, std=0.016985\n",
      "Global init param 9: shape=(256,), mean=0.001210, std=0.016608\n",
      "Global init param 10: shape=(256, 256, 3, 3), mean=0.000009, std=0.012009\n",
      "Global init param 11: shape=(256,), mean=-0.000338, std=0.012563\n",
      "Global init param 12: shape=(1024, 4096), mean=-0.000006, std=0.009020\n",
      "Global init param 13: shape=(1024,), mean=-0.000122, std=0.009008\n",
      "Global init param 14: shape=(512, 1024), mean=-0.000047, std=0.018037\n",
      "Global init param 15: shape=(512,), mean=-0.000623, std=0.017951\n",
      "Global init param 16: shape=(10, 512), mean=0.000112, std=0.025984\n",
      "Global init param 17: shape=(10,), mean=0.000054, std=0.016862\n",
      "\n",
      "==== Server-side evaluation for round 0 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 0.07207727766036988, {'accuracy': 0.1}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluation results - Loss: 0.0721, Accuracy: 0.1000\n",
      "\u001b[36m(ClientAppActor pid=27171)\u001b[0m [Client 0] fit, config: {'proximal_mu': 0.1}\n",
      "\u001b[36m(ClientAppActor pid=27171)\u001b[0m Epoch 1: train loss 0.06186716631054878, accuracy 0.23970599264981624\n",
      "\u001b[36m(ClientAppActor pid=27170)\u001b[0m [Client 1] fit, config: {'proximal_mu': 0.1}\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "net = Net().to(DEVICE)\n",
    "\n",
    "_, _, testloader = load_datasets(0, NUM_PARTITIONS)\n",
    "\n",
    "evaluate_fn = get_evaluate_fn(testloader, net)\n",
    "\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Configure the server for just 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=NUM_OF_ROUNDS)\n",
    "    return ServerAppComponents(\n",
    "        config=config,\n",
    "        strategy=ModifiedFedProx(proximal_mu=0.1, evaluate_fn=evaluate_fn),\n",
    "    )\n",
    "\n",
    "server = ServerApp(server_fn=server_fn)\n",
    "\n",
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_PARTITIONS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results/fed_prox_result.p', 'wb') as file:\n",
    "    pickle.dump(fed_prox_result, file)\n",
    "\n",
    "with open(f'results/fed_prox_model_results.p', 'wb') as file:\n",
    "    pickle.dump(fed_prox_model_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_prox_rounds = list(fed_prox_result.keys())\n",
    "fed_prox_sizes = [fed_prox_result[round][\"total_size\"] for round in fed_prox_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_prox_rounds, fed_prox_sizes, marker='o', linestyle='-', color='b', label='FedProx')\n",
    "# plt.plot(fed_part_avg_rounds, fed_part_avg_sizes, marker='o', linestyle='-', color='r', label='FedPartAvg')\n",
    "# plt.plot(fed_avg_rounds, fed_avg_sizes, marker='o', linestyle='-', color='g', label='FedAvg')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Total Size of Parameters (bytes)')\n",
    "# plt.title('Total Size of Parameters for Each Round')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# fed_prox_losses = [fed_prox_result[round][\"total_loss\"] for round in fed_prox_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_prox_rounds, fed_prox_losses, marker='o', linestyle='-', color='b', label='FedProx')\n",
    "# plt.plot(fed_part_avg_rounds, fed_part_avg_losses, marker='o', linestyle='-', color='r', label='FedPartAvg')\n",
    "# plt.plot(fed_avg_rounds, fed_avg_losses, marker='o', linestyle='-', color='g', label='FedAvg')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Total Loss')\n",
    "# plt.title('Total Loss for Each Round')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "\n",
    "# fed_prox_model_rounds = list(fed_prox_model_results.keys())\n",
    "# fed_prox_accuracies = [fed_prox_model_results[round][\"global_metrics\"][\"accuracy\"] for round in fed_prox_model_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# # plt.plot(fed_part_prox_model_rounds, fed_part_prox_accuracies, marker='o', linestyle='-', color='b', label='FedPartProx')\n",
    "# plt.plot(fed_part_avg_model_rounds, fed_part_avg_accuracies, marker='o', linestyle='-', color='r', label='FedPartAvg')\n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_accuracies, marker='o', linestyle='-', color='g', label='FedAvg')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Accuracy for Each Round')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# fed_prox_global_losses = [fed_prox_model_results[round][\"global_loss\"] for round in fed_prox_model_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# # plt.plot(fed_part_prox_model_rounds, fed_part_prox_global_losses, marker='o', linestyle='-', color='b', label='FedPartProx')\n",
    "# plt.plot(fed_part_avg_model_rounds, fed_part_avg_global_losses, marker='o', linestyle='-', color='r', label='FedPartAvg')   \n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_global_losses, marker='o', linestyle='-', color='g', label='FedAvg')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Loss for Each Round')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedMoon experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "class FedMoonNoFreezeFlowerClient(NumPyClient):\n",
    "    def __init__(self, partition_id, net, trainloader, valloader):\n",
    "        self.partition_id = partition_id\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.model_dir = \"models\"\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.partition_id}] get_parameters\")\n",
    "        parameters = get_parameters(self.net)\n",
    "        trainable_layer = config[\"trainable_layers\"]\n",
    "        self._save_model_state()\n",
    "        \n",
    "        if trainable_layer == -1:\n",
    "            return parameters\n",
    "        \n",
    "        trained_layer = [parameters[trainable_layer*2], parameters[trainable_layer*2 +1]]\n",
    "        return trained_layer\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] fit, config: {config}\")\n",
    "\n",
    "        # load previous model\n",
    "        if not os.path.exists(os.path.join(self.model_dir, str(self.partition_id))):\n",
    "            prev_model = copy.deepcopy(self.net)\n",
    "        else:\n",
    "            # initialise and load params from model_dir\n",
    "            prev_model = type(self.net)() \n",
    "            prev_model.load_state_dict(\n",
    "                torch.load(\n",
    "                    os.path.join(self.model_dir, str(self.partition_id), \"prev_net.pt\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # update params for current model (loading global params)\n",
    "        set_parameters(self.net, parameters)\n",
    "\n",
    "        # create global model (same params that were just loaded)\n",
    "        global_model = type(self.net)()\n",
    "        global_model.load_state_dict(self.net.state_dict())\n",
    "        global_model.to(DEVICE)\n",
    "        \n",
    "        train_moon(self.net, self.trainloader, global_model, prev_model, EPOCHS, 5, 0.5)\n",
    "\n",
    "        # save current model \n",
    "        if not os.path.exists(os.path.join(self.model_dir, str(self.partition_id))):\n",
    "            os.makedirs(os.path.join(self.model_dir, str(self.partition_id)))\n",
    "        torch.save(\n",
    "            self.net.state_dict(),\n",
    "            os.path.join(self.model_dir, str(self.partition_id), \"prev_net.pt\"),\n",
    "        )\n",
    "\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test_moon(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "def client_fn(context: Context) -> Client:\n",
    "    net = MoonNet().to(DEVICE)\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "    trainloader, valloader, _ = load_datasets(partition_id, num_partitions)\n",
    "    return FedMoonNoFreezeFlowerClient(partition_id, net, trainloader, valloader).to_client()\n",
    "\n",
    "\n",
    "# Create the ClientApp\n",
    "client = ClientApp(client_fn=client_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import sys\n",
    "\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.strategy.aggregate import aggregate, weighted_loss_avg\n",
    "\n",
    "def get_parameters_size(params: Parameters) -> int:\n",
    "    size = sys.getsizeof(params)  # Base size of the dataclass instance\n",
    "    size += sys.getsizeof(params.tensor_type)  # Size of the string\n",
    "    size += sys.getsizeof(params.tensors)  # Size of the list container\n",
    "    size += sum(sys.getsizeof(tensor) for tensor in params.tensors)  # Size of each bytes object\n",
    "    return size\n",
    "\n",
    "fed_moon_no_freeze_result = {}\n",
    "fed_moon_model_no_freeze_results = {}\n",
    "\n",
    "# basically same as normal FedAvg, just added freezing and modified result dict names\n",
    "class FedMoonNoFreeze(Strategy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fraction_fit: float = 1.0,\n",
    "        fraction_evaluate: float = 1.0,\n",
    "        min_fit_clients: int = 2,\n",
    "        min_evaluate_clients: int = 2,\n",
    "        min_available_clients: int = 2,\n",
    "        evaluate_fn: Optional[\n",
    "            Callable[\n",
    "                [int, NDArrays, dict[str, Scalar]],\n",
    "                Optional[tuple[float, dict[str, Scalar]]],\n",
    "            ]\n",
    "        ] = None,\n",
    "        on_fit_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,\n",
    "        on_evaluate_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,\n",
    "        accept_failures: bool = True,\n",
    "        initial_parameters: Optional[Parameters] = None,\n",
    "        fit_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
    "        evaluate_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
    "        inplace: bool = True,\n",
    "        layer_update_strategy: str = \"sequential\",\n",
    "        \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.fraction_fit = fraction_fit\n",
    "        self.fraction_evaluate = fraction_evaluate\n",
    "        self.min_fit_clients = min_fit_clients\n",
    "        self.min_evaluate_clients = min_evaluate_clients\n",
    "        self.min_available_clients = min_available_clients\n",
    "        self.evaluate_fn = evaluate_fn\n",
    "        self.on_fit_config_fn = on_fit_config_fn\n",
    "        self.on_evaluate_config_fn = on_evaluate_config_fn\n",
    "        self.accept_failures = accept_failures\n",
    "        self.initial_parameters = initial_parameters\n",
    "        self.fit_metrics_aggregation_fn = fit_metrics_aggregation_fn\n",
    "        self.evaluate_metrics_aggregation_fn = evaluate_metrics_aggregation_fn\n",
    "        self.inplace = inplace\n",
    "        self.layer_training_sequence = []\n",
    "        self.training_sequence_index = 0\n",
    "        self.latest_parameters = initial_parameters\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"FedMoon\"\n",
    "    \n",
    "    def num_fit_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Return sample size and required number of clients.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_fit)\n",
    "        return max(num_clients, self.min_fit_clients), self.min_available_clients\n",
    "\n",
    "    def num_evaluation_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Use a fraction of available clients for evaluation.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_evaluate)\n",
    "        return max(num_clients, self.min_evaluate_clients), self.min_available_clients\n",
    "   \n",
    "    def initialize_parameters(\n",
    "        self, client_manager: ClientManager\n",
    "    ) -> Optional[Parameters]:\n",
    "        \"\"\"Initialize global model parameters.\"\"\"\n",
    "        net = Net()\n",
    "        ndarrays = get_parameters(net)\n",
    "        return ndarrays_to_parameters(ndarrays)\n",
    "\n",
    "    def evaluate(\n",
    "        self, server_round: int, parameters: Parameters\n",
    "    ) -> Optional[tuple[float, dict[str, Scalar]]]:\n",
    "        \"\"\"Evaluate model parameters using an evaluation function.\"\"\"\n",
    "        if self.evaluate_fn is None:\n",
    "            # No evaluation function provided\n",
    "            return None\n",
    "        parameters_ndarrays = parameters_to_ndarrays(parameters)\n",
    "        eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})\n",
    "        if eval_res is None:\n",
    "            return None\n",
    "        loss, metrics = eval_res\n",
    "\n",
    "        if server_round in fed_moon_model_no_freeze_results:\n",
    "            expand_fed_moon_no_freeze_result= {**fed_moon_model_no_freeze_results[server_round], \"global_loss\": loss, \"global_metrics\": metrics}\n",
    "        else:\n",
    "            expand_fed_moon_no_freeze_result= {\"global_loss\": loss, \"global_metrics\": metrics}\n",
    "\n",
    "        fed_moon_model_no_freeze_results[server_round] = expand_fed_moon_no_freeze_result\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "\n",
    "    def configure_fit(\n",
    "        # includes layer freezing\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, FitIns]]:\n",
    "        \"\"\"Configure the next round of training.\"\"\"\n",
    "        config = {}\n",
    "        \n",
    "        sample_size, min_num_clients = self.num_fit_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "        \n",
    "        fit_configurations = []\n",
    "        for idx, client in enumerate(clients):\n",
    "            fit_configurations.append((client, FitIns(parameters, config)))\n",
    "\n",
    "        self.training_sequence_index = self.training_sequence_index + 1\n",
    "        \n",
    "        return fit_configurations\n",
    "    \n",
    "    def configure_evaluate(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, EvaluateIns]]:\n",
    "        \"\"\"Configure the next round of evaluation.\"\"\"\n",
    "        if self.fraction_evaluate == 0.0:\n",
    "            return []\n",
    "        config = {}\n",
    "        evaluate_ins = EvaluateIns(parameters, config)\n",
    "\n",
    "        # Sample clients\n",
    "        sample_size, min_num_clients = self.num_evaluation_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "\n",
    "        # Return client/config pairs\n",
    "        return [(client, evaluate_ins) for client in clients]\n",
    "\n",
    "\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate fit results using weighted average.\"\"\"\n",
    "\n",
    "        # get size of parameters in bytes\n",
    "        total_size = 0\n",
    "        for client, fit_res in results:\n",
    "            total_size += get_parameters_size(fit_res.parameters) * 2\n",
    "        \n",
    "        if server_round in fed_moon_no_freeze_result:\n",
    "            expand_fed_moon_no_freeze_result= {**fed_moon_no_freeze_result[server_round], \"total_size\": total_size}\n",
    "        else:\n",
    "            expand_fed_moon_no_freeze_result= {\"total_size\": total_size}\n",
    "\n",
    "        fed_moon_no_freeze_result[server_round] = expand_fed_moon_no_freeze_result\n",
    "\n",
    "        weights_results = [\n",
    "            (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)\n",
    "            for _, fit_res in results\n",
    "        ]\n",
    "        \n",
    "        aggregated_weights = aggregate(weights_results)\n",
    "        \n",
    "        self.latest_parameters = ndarrays_to_parameters(aggregated_weights)\n",
    "\n",
    "        metrics_aggregated = {}\n",
    "        return self.latest_parameters, metrics_aggregated\n",
    "\n",
    "    \n",
    "\n",
    "    def aggregate_evaluate(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, EvaluateRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
    "    ) -> Tuple[Optional[float], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate evaluation losses using weighted average.\"\"\"\n",
    "\n",
    "        if not results:\n",
    "            return None, {}\n",
    "\n",
    "        total_loss = 0\n",
    "        for _, evaluate_res in results:\n",
    "            total_loss += evaluate_res.loss \n",
    "\n",
    "\n",
    "        if server_round in fed_moon_no_freeze_result:\n",
    "            expand_fed_moon_no_freeze_result= {**fed_moon_no_freeze_result[server_round], \"total_loss\": total_loss}\n",
    "        else:\n",
    "            expand_fed_moon_no_freeze_result= {\"total_loss\": total_loss}\n",
    "\n",
    "        fed_moon_no_freeze_result[server_round] = expand_fed_moon_no_freeze_result\n",
    "\n",
    "        loss_aggregated = weighted_loss_avg(\n",
    "            [\n",
    "                (evaluate_res.num_examples, evaluate_res.loss)\n",
    "                for _, evaluate_res in results\n",
    "            ]\n",
    "        )\n",
    "        metrics_aggregated = {}\n",
    "        return loss_aggregated, metrics_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train FedMOON\n",
    "\n",
    "\n",
    "_, _, testloader = load_datasets(0, NUM_PARTITIONS)\n",
    "net = MoonNet().to(DEVICE)\n",
    "evaluate_fn = get_evaluate_fn_moon(testloader, net)\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Configure the server for just 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=NUM_OF_ROUNDS)\n",
    "    return ServerAppComponents(\n",
    "        config=config,\n",
    "        strategy=FedMoonNoFreeze(\n",
    "            evaluate_fn=evaluate_fn\n",
    "        )\n",
    "    )\n",
    "\n",
    "server = ServerApp(server_fn=server_fn)\n",
    "\n",
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_PARTITIONS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results/fed_moon_no_freeze_result.p', 'wb') as file:\n",
    "    pickle.dump(fed_moon_no_freeze_result, file)\n",
    "\n",
    "with open(f'results/fed_moon_model_no_freeze_results.p', 'wb') as file:\n",
    "    pickle.dump(fed_moon_model_no_freeze_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fed_moon_rounds = list(fed_moon_no_freeze_result.keys())\n",
    "# fed_moon_sizes = [fed_moon_no_freeze_result[round][\"total_size\"] for round in fed_moon_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_avg_rounds, fed_avg_sizes, marker='o', linestyle='-', color='b', label='FedAvg')\n",
    "# plt.plot(fed_part_avg_rounds, fed_part_avg_sizes, marker='o', linestyle='-', color='r', label='FedPartAvg')\n",
    "# plt.plot(fed_prox_rounds, fed_prox_sizes, marker='o', linestyle='-', color='g', label='FedProx')\n",
    "# plt.plot(fed_part_prox_rounds, fed_part_prox_sizes, marker='o', linestyle='-', color='y', label='FedPartProx')\n",
    "# plt.plot(fed_moon_rounds, fed_moon_sizes, marker='o', linestyle='-', color='c', label='FedMoon')\n",
    "# plt.plot(fed_part_moon_rounds, fed_part_moon_sizes, marker='o', linestyle='-', color='purple', label='FedPartMoon')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Communication Cost (bytes)')\n",
    "# plt.title('Communication Cost for Each Round')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# fed_moon_losses = [fed_moon_no_freeze_result[round][\"total_loss\"] for round in fed_moon_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_avg_rounds, fed_avg_losses, marker='o', linestyle='-', color='b', label='FedAvg')\n",
    "# plt.plot(fed_part_avg_rounds, fed_part_avg_losses, marker='o', linestyle='-', color='r', label='FedPartAvg')\n",
    "# plt.plot(fed_prox_rounds, fed_prox_losses, marker='o', linestyle='-', color='g', label='FedProx')\n",
    "# plt.plot(fed_part_prox_rounds, fed_part_prox_losses, marker='o', linestyle='-', color='y', label='FedPartProx')\n",
    "# plt.plot(fed_moon_rounds, fed_moon_losses, marker='o', linestyle='-', color='c', label='FedMoon')\n",
    "# plt.plot(fed_part_moon_rounds, fed_part_moon_losses, marker='o', linestyle='-', color='purple', label='FedPartMoon')\n",
    "\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Aggregate Client Loss for Each Round')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# fed_moon_model_rounds = list(fed_moon_model_no_freeze_results.keys())\n",
    "# fed_moon_accuracies = [fed_moon_model_no_freeze_results[round][\"global_metrics\"][\"accuracy\"] for round in fed_moon_model_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_accuracies, marker='o', linestyle='-', color='b', label='FedAvg')\n",
    "# plt.plot(fed_part_avg_model_rounds, fed_part_avg_accuracies, marker='o', linestyle='-', color='r', label='FedPartAvg')\n",
    "# plt.plot(fed_prox_model_rounds, fed_prox_accuracies, marker='o', linestyle='-', color='g', label='FedProx')\n",
    "# plt.plot(fed_part_prox_model_rounds, fed_part_prox_accuracies, marker='o', linestyle='-', color='y', label='FedPartProx')\n",
    "# plt.plot(fed_moon_model_rounds, fed_moon_accuracies, marker='o', linestyle='-', color='c', label='FedMoon')\n",
    "# plt.plot(fed_part_moon_model_rounds, fed_part_moon_accuracies, marker='o', linestyle='-', color='purple', label='FedPartMoon')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Global Model Accuracy for Each Round')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# fed_moon_global_losses = [fed_moon_model_no_freeze_results[round][\"global_loss\"] for round in fed_moon_model_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_global_losses, marker='o', linestyle='-', color='b', label='FedAvg')\n",
    "# plt.plot(fed_part_avg_model_rounds, fed_part_avg_global_losses, marker='o', linestyle='-', color='r', label='FedPartAvg')\n",
    "# plt.plot(fed_prox_model_rounds, fed_prox_global_losses, marker='o', linestyle='-', color='g', label='FedProx')\n",
    "# plt.plot(fed_part_prox_model_rounds, fed_part_prox_global_losses, marker='o', linestyle='-', color='y', label='FedPartProx')\n",
    "# plt.plot(fed_moon_model_rounds, fed_moon_global_losses, marker='o', linestyle='-', color='c', label='FedMoon')\n",
    "# plt.plot(fed_part_moon_model_rounds, fed_part_moon_global_losses, marker='o', linestyle='-', color='purple', label='FedPartMoon')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Global Model Loss for Each Round')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
