{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q flwr[simulation] flwr-datasets[vision] torch torchvision matplotlib\n",
    "! pip install -U ipywidgets\n",
    "! pip install numpy==1.26.4\n",
    "! pip install urllib3==1.26.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on mps\n",
      "Flower 1.15.1 / PyTorch 2.6.0\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple, Union, Callable\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import DataLoader\n",
    "from flwr.server.strategy import Strategy\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Metrics, Context, Status, GetParametersRes, Parameters, GetParametersIns, MetricsAggregationFn,NDArrays,Scalar\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents \n",
    "from flwr.server.strategy import FedAvg, FedProx\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    "    ParametersRecord,\n",
    "    array_from_numpy\n",
    ")\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.strategy.aggregate import aggregate, weighted_loss_avg\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = \"mps\"\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def load_datasets(partition_id, num_partitions: int):\n",
    "    fds = FederatedDataset(dataset=\"cifar10\", partitioners={\"train\": num_partitions})\n",
    "    partition = fds.load_partition(partition_id)\n",
    "    # Divide data on each node: 80% train, 20% test\n",
    "    partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
    "    pytorch_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        \n",
    "        batch[\"img\"] = [pytorch_transforms(img) for img in batch[\"img\"]]\n",
    "        return batch\n",
    "\n",
    "    partition_train_test = partition_train_test.with_transform(apply_transforms)\n",
    "    trainloader = DataLoader(partition_train_test[\"train\"], batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(partition_train_test[\"test\"], batch_size=32)\n",
    "    testset = fds.load_split(\"test\").with_transform(apply_transforms)\n",
    "    testloader = DataLoader(testset, batch_size=32)\n",
    "    return trainloader, valloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(256*4*4, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 256*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MoonNet(nn.Module):\n",
    "    \"\"\"Returns both the representation (penultimate layer output) and classification\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super(MoonNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(256*4*4, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 256*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        representation = x.clone()\n",
    "        classification = self.fc3(x)\n",
    "        return representation, classification\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "# def set_parameters(net, parameters):\n",
    "#     \"\"\"Set model parameters from a list of NumPy arrays.\"\"\"\n",
    "#     # Get current state dict to validate shape compatibility\n",
    "#     current_state = OrderedDict(net.state_dict())\n",
    "    \n",
    "#     # Create parameter dictionary with explicit device placement and dtype preservation\n",
    "#     params_dict = {}\n",
    "#     for (name, current_tensor), param_array in zip(current_state.items(), parameters):\n",
    "#         # Validate shape compatibility\n",
    "#         if current_tensor.shape != torch.Size(param_array.shape):\n",
    "#             raise ValueError(f\"Shape mismatch for {name}: expected {current_tensor.shape}, got {param_array.shape}\")\n",
    "        \n",
    "#         # Create tensor with same dtype as original and place on correct device\n",
    "#         param_tensor = torch.tensor(param_array, dtype=current_tensor.dtype, device=current_tensor.device)\n",
    "        \n",
    "#         # Validate the tensor has valid values (not NaN or Inf)\n",
    "#         if not torch.all(torch.isfinite(param_tensor)):\n",
    "#             print(f\"WARNING: Parameter {name} contains NaN or Inf values. Clipping to valid range.\")\n",
    "#             param_tensor = torch.nan_to_num(param_tensor, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "            \n",
    "#         params_dict[name] = param_tensor\n",
    "    \n",
    "#     # Create ordered dict for loading\n",
    "#     state_dict = OrderedDict(params_dict)\n",
    "    \n",
    "#     # Print stats of a few parameters to debug\n",
    "#     for name in list(state_dict.keys())[:3]:  # Just print first 3 params\n",
    "#         tensor = state_dict[name]\n",
    "#         print(f\"Updated {name}: shape={tensor.shape}, mean={tensor.mean().item():.6f}, std={tensor.std().item():.6f}\")\n",
    "    \n",
    "#     # Load state dict with strict flag to ensure all parameters are set\n",
    "#     net.load_state_dict(state_dict, strict=True)\n",
    "    \n",
    "#     # Validate model has valid parameters after loading\n",
    "#     for name, param in net.named_parameters():\n",
    "#         if not torch.all(torch.isfinite(param)):\n",
    "#             raise RuntimeError(f\"Parameter {name} contains NaN or Inf values after loading.\")\n",
    "\n",
    "def set_parameters(net, parameters, trainable_layers=-1):\n",
    "    \"\"\"Set model parameters from a list of NumPy arrays.\"\"\"\n",
    "    current_state = OrderedDict(net.state_dict())\n",
    "    \n",
    "    if trainable_layers == -1:\n",
    "        # Update all parameters\n",
    "        params_dict = zip(current_state.keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "        net.load_state_dict(state_dict, strict=True)\n",
    "    else:\n",
    "        # Only update the specified layer's parameters\n",
    "        # Convert current state to numpy arrays\n",
    "        numpy_state = [param.cpu().numpy() for param in current_state.values()]\n",
    "        \n",
    "        # Update the specific indices with new parameters\n",
    "        numpy_state[trainable_layers*2] = parameters[0]\n",
    "        numpy_state[trainable_layers*2 + 1] = parameters[1]\n",
    "        \n",
    "        # Convert back to torch and update state dict\n",
    "        for idx, key in enumerate(current_state.keys()):\n",
    "            current_state[key] = torch.from_numpy(numpy_state[idx])\n",
    "        \n",
    "        net.load_state_dict(current_state, strict=True)\n",
    "\n",
    "\n",
    "# def set_parameters(net, parameters: List[np.ndarray]):\n",
    "#     params_dict = zip(net.state_dict().keys(), parameters)\n",
    "#     state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "#     net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "def train(net, trainloader, epochs: int):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for batch in trainloader:\n",
    "            images, labels = batch[\"img\"], batch[\"label\"]\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "        \n",
    "def proxima_train(net, trainloader, epochs: int, proximal_mu:float, global_params:List[torch.Tensor]):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for batch in trainloader:\n",
    "            images, labels = batch[\"img\"], batch[\"label\"]\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "\n",
    "            proximal_term = 0.0\n",
    "            for local_weights, global_weights in zip(net.parameters(), global_params):\n",
    "                proximal_term += (local_weights - global_weights).norm(2)\n",
    "            loss = criterion(net(images), labels) + (proximal_mu / 2) * proximal_term\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "\n",
    "\n",
    "def train_moon(net,train_loader, global_net,previous_net, epochs, mu, temperature):\n",
    "    \"\"\"Training function for MOON.\"\"\"\n",
    "    print(f\"Started training moon\")\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    previous_net.eval()\n",
    "    global_net.eval()\n",
    "\n",
    "    cnt = 0\n",
    "    cos = torch.nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss_collector = []\n",
    "        epoch_loss1_collector = []\n",
    "        epoch_loss2_collector = []\n",
    "        for batch in train_loader:\n",
    "            x, target = batch[\"img\"], batch[\"label\"]\n",
    "            x, target = x.to(DEVICE), target.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # pro1 is the representation by the current model (Line 14 of Algorithm 1)\n",
    "            pro1, out = net(x)\n",
    "            # pro2 is the representation by the global model (Line 15 of Algorithm 1)\n",
    "            # pro3 is the representation by the previous model (Line 16 of Algorithm 1)\n",
    "            with torch.no_grad():\n",
    "                pro2, _ = global_net(x)\n",
    "                pro3, _ = previous_net(x)\n",
    "\n",
    "            # posi is the positive pair\n",
    "            posi = cos(pro1, pro2)\n",
    "            logits = posi.reshape(-1, 1)\n",
    "\n",
    "            # nega is the negative pair\n",
    "            nega = cos(pro1, pro3)\n",
    "            logits = torch.cat((logits, nega.reshape(-1, 1)), dim=1)\n",
    "\n",
    "            previous_net.to(\"cpu\")\n",
    "            logits /= temperature\n",
    "            labels = torch.zeros(x.size(0)).to(DEVICE).long()\n",
    "\n",
    "            # compute the model-contrastive loss (Line 17 of Algorithm 1)\n",
    "            loss2 = mu * criterion(logits, labels)\n",
    "\n",
    "            # compute the cross-entropy loss (Line 13 of Algorithm 1)\n",
    "            loss1 = criterion(out, target)\n",
    "\n",
    "            # compute the loss (Line 18 of Algorithm 1)\n",
    "            loss = loss1 + loss2\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cnt += 1\n",
    "            epoch_loss_collector.append(loss.item())\n",
    "            epoch_loss1_collector.append(loss1.item())\n",
    "            epoch_loss2_collector.append(loss2.item())\n",
    "\n",
    "        epoch_loss = sum(epoch_loss_collector) / len(epoch_loss_collector)\n",
    "        epoch_loss1 = sum(epoch_loss1_collector) / len(epoch_loss1_collector)\n",
    "        epoch_loss2 = sum(epoch_loss2_collector) / len(epoch_loss2_collector)\n",
    "        print(\n",
    "            \"Epoch: %d Loss: %f Loss1: %f Loss2: %f\"\n",
    "            % (epoch, epoch_loss, epoch_loss1, epoch_loss2)\n",
    "        )\n",
    "\n",
    "\n",
    "def test_moon(net, testloader):\n",
    "    \"\"\"\n",
    "    Evaluate the network on the entire test set.\n",
    "    Same as the regular test, but using the MoonNet \n",
    "    (where the output is a tuple of (representation, classification) )\n",
    "    \"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images, labels = batch[\"img\"], batch[\"label\"]\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            _, outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images, labels = batch[\"img\"], batch[\"label\"]\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy\n",
    "\n",
    "# def freeze_layers(model: torch.nn.Module, trainable_layers: int) -> None:\n",
    "#         \"\"\"Freeze specified layers of the model.\"\"\"\n",
    "#         for idx, (name, param) in enumerate(model.named_parameters()):\n",
    "#             if idx == trainable_layers or trainable_layers == -1:\n",
    "#                 param.requires_grad = True\n",
    "#             else:\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "def freeze_layers(model: torch.nn.Module, trainable_layers: int) -> None:\n",
    "        \"\"\"Freeze specified layers of the model.\"\"\"\n",
    "        trainable_layers_set = []\n",
    "        if trainable_layers == -1:\n",
    "            trainable_layers_set = [-1]\n",
    "        else:\n",
    "            trainable_layers_set = [trainable_layers *2, trainable_layers *2 +1]\n",
    "\n",
    "        for idx, (name, param) in enumerate(model.named_parameters()):\n",
    "            \n",
    "            if idx in trainable_layers_set or trainable_layers_set[0] == -1:\n",
    "                param.requires_grad = True\n",
    "                print(f\"layer index is {idx} and name{name} is trainabe\")\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                print(f\"layer index is {idx} and name{name} is frozen\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rounds: 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NETWORK_LEN = len(Net().state_dict().keys()) //2 \n",
    "EPOCHS = 8\n",
    "NUM_PARTITIONS = 3\n",
    "NUM_OF_CYCLES  = 1\n",
    "NUM_OF_FULL_UPDATES_BETWEEN_CYCLES = 2\n",
    "NUM_OF_ROUNDS = (NUM_OF_CYCLES * NUM_OF_FULL_UPDATES_BETWEEN_CYCLES) + (NUM_OF_CYCLES * NETWORK_LEN *2)\n",
    "print(f\"Number of rounds: {NUM_OF_ROUNDS}\")\n",
    "backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.common import NDArrays, Scalar\n",
    "import sys\n",
    "# TEMPORARY\n",
    "def get_evaluate_fn(\n",
    "    testloader: DataLoader,\n",
    "    net: torch.nn.Module,\n",
    ") -> Callable[[int, NDArrays, Dict[str, Scalar]], Optional[Tuple[float, Dict[str, Scalar]]]]:\n",
    "    \"\"\"Return an evaluation function for server-side evaluation.\"\"\"\n",
    "\n",
    "    def evaluate(\n",
    "        server_round: int, parameters: NDArrays, config: Dict[str, Scalar]\n",
    "    ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "        \"\"\"Use the entire test set for evaluation.\"\"\"\n",
    "        \n",
    "        # Copy model parameters to avoid modifying the original\n",
    "        net_copy = copy.deepcopy(net)\n",
    "        \n",
    "        # Update model with the latest parameters\n",
    "        params_dict = zip(net_copy.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "        net_copy.load_state_dict(state_dict, strict=True)\n",
    "        \n",
    "        net_copy.to(DEVICE)\n",
    "        net_copy.eval()\n",
    "\n",
    "        # Test the model\n",
    "        loss, accuracy = test(net_copy, testloader)\n",
    "        \n",
    "        # Return loss and metrics\n",
    "        return loss, {\"accuracy\": accuracy}\n",
    "\n",
    "    return evaluate\n",
    "\n",
    "# def get_evaluate_fn(\n",
    "#     testloader: DataLoader,\n",
    "#     net: torch.nn.Module,\n",
    "# ) -> Callable[[int, NDArrays, Dict[str, Scalar]], Optional[Tuple[float, Dict[str, Scalar]]]]:\n",
    "#     \"\"\"Return an evaluation function for server-side evaluation.\"\"\"\n",
    "    \n",
    "#     # Store previous parameters to check if they're changing\n",
    "#     previous_params = None\n",
    "    \n",
    "#     def evaluate(\n",
    "#         server_round: int, parameters: NDArrays, config: Dict[str, Scalar]\n",
    "#     ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "#         \"\"\"Use the entire test set for evaluation.\"\"\"\n",
    "#         nonlocal previous_params\n",
    "        \n",
    "#         print(f\"\\n==== Server-side evaluation for round {server_round} ====\")\n",
    "        \n",
    "#         # Check if parameters changed from previous round\n",
    "#         if previous_params is not None:\n",
    "#             param_change = False\n",
    "#             for i, (prev, curr) in enumerate(zip(previous_params, parameters)):\n",
    "#                 diff = np.abs(prev - curr).mean()\n",
    "#                 if diff > 1e-6:\n",
    "#                     param_change = True\n",
    "#                     print(f\"  Parameter {i}: Changed by {diff:.6f}\")\n",
    "            \n",
    "#             if not param_change:\n",
    "#                 print(\"  WARNING: Parameters haven't changed from previous round!\")\n",
    "        \n",
    "#         # Update previous params for next round\n",
    "#         previous_params = [p.copy() for p in parameters]\n",
    "        \n",
    "#         # Copy model parameters to avoid modifying the original\n",
    "#         net_copy = copy.deepcopy(net)\n",
    "        \n",
    "#         # Update model with the latest parameters\n",
    "#         params_dict = zip(net_copy.state_dict().keys(), parameters)\n",
    "#         state_dict = OrderedDict({k: torch.tensor(v, device=DEVICE) for k, v in params_dict})\n",
    "        \n",
    "#         # Check if state dict keys match model keys\n",
    "#         model_keys = set(net_copy.state_dict().keys())\n",
    "#         params_keys = set(state_dict.keys())\n",
    "#         if model_keys != params_keys:\n",
    "#             print(f\"  WARNING: Key mismatch between model and parameters!\")\n",
    "#             print(f\"  Missing in params: {model_keys - params_keys}\")\n",
    "#             print(f\"  Extra in params: {params_keys - model_keys}\")\n",
    "        \n",
    "#         net_copy.load_state_dict(state_dict, strict=True)\n",
    "#         net_copy.to(DEVICE)\n",
    "#         net_copy.eval()\n",
    "        \n",
    "#         # Test the model\n",
    "#         loss, accuracy = test(net_copy, testloader)\n",
    "#         print(f\"  Evaluation results - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "#         # Return loss and metrics\n",
    "#         return loss, {\"accuracy\": accuracy}\n",
    "    \n",
    "#     return evaluate\n",
    "\n",
    "\n",
    "\n",
    "def get_evaluate_fn_moon(\n",
    "    testloader: DataLoader,\n",
    "    net: torch.nn.Module,\n",
    ") -> Callable[[int, NDArrays, Dict[str, Scalar]], Optional[Tuple[float, Dict[str, Scalar]]]]:\n",
    "    \"\"\"Return an evaluation function for server-side evaluation.\"\"\"\n",
    "\n",
    "    def evaluate(\n",
    "        server_round: int, parameters: NDArrays, config: Dict[str, Scalar]\n",
    "    ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "        \"\"\"Use the entire test set for evaluation.\"\"\"\n",
    "        \n",
    "        # Copy model parameters to avoid modifying the original\n",
    "        net_copy = copy.deepcopy(net)\n",
    "        \n",
    "        # Update model with the latest parameters\n",
    "        params_dict = zip(net_copy.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "        net_copy.load_state_dict(state_dict, strict=True)\n",
    "        \n",
    "        net_copy.to(DEVICE)\n",
    "        net_copy.eval()\n",
    "\n",
    "        # Test the model\n",
    "        loss, accuracy = test_moon(net_copy, testloader)\n",
    "        \n",
    "        # Return loss and metrics\n",
    "        return loss, {\"accuracy\": accuracy}\n",
    "\n",
    "    return evaluate\n",
    "\n",
    "\n",
    "def get_parameters_size(params: Parameters) -> int:\n",
    "    size = sys.getsizeof(params)  # Base size of the dataclass instance\n",
    "    size += sys.getsizeof(params.tensor_type)  # Size of the string\n",
    "    size += sys.getsizeof(params.tensors)  # Size of the list container\n",
    "    size += sum(sys.getsizeof(tensor) for tensor in params.tensors)  # Size of each bytes object\n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.strategy.aggregate import aggregate, weighted_loss_avg\n",
    "\n",
    "\n",
    "fed_avg_result = {}\n",
    "fed_avg_model_results = {}\n",
    "\n",
    "class ModifiedFedAvg(Strategy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fraction_fit: float = 1.0,\n",
    "        fraction_evaluate: float = 1.0,\n",
    "        min_fit_clients: int = 2,\n",
    "        min_evaluate_clients: int = 2,\n",
    "        min_available_clients: int = 2,\n",
    "        evaluate_fn: Optional[\n",
    "            Callable[\n",
    "                [int, NDArrays, dict[str, Scalar]],\n",
    "                Optional[tuple[float, dict[str, Scalar]]],\n",
    "            ]\n",
    "        ] = None,\n",
    "        on_fit_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,\n",
    "        on_evaluate_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,\n",
    "        accept_failures: bool = True,\n",
    "        initial_parameters: Optional[Parameters] = None,\n",
    "        fit_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
    "        evaluate_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
    "        inplace: bool = True,\n",
    "        layer_update_strategy: str = \"sequential\",\n",
    "        \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.fraction_fit = fraction_fit\n",
    "        self.fraction_evaluate = fraction_evaluate\n",
    "        self.min_fit_clients = min_fit_clients\n",
    "        self.min_evaluate_clients = min_evaluate_clients\n",
    "        self.min_available_clients = min_available_clients\n",
    "        self.evaluate_fn = evaluate_fn\n",
    "        self.on_fit_config_fn = on_fit_config_fn\n",
    "        self.on_evaluate_config_fn = on_evaluate_config_fn\n",
    "        self.accept_failures = accept_failures\n",
    "        self.initial_parameters = initial_parameters\n",
    "        self.fit_metrics_aggregation_fn = fit_metrics_aggregation_fn\n",
    "        self.evaluate_metrics_aggregation_fn = evaluate_metrics_aggregation_fn\n",
    "        self.inplace = inplace\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"FedPartAvg\"\n",
    "    \n",
    "\n",
    "    def num_fit_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Return sample size and required number of clients.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_fit)\n",
    "        return max(num_clients, self.min_fit_clients), self.min_available_clients\n",
    "\n",
    "    def num_evaluation_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Use a fraction of available clients for evaluation.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_evaluate)\n",
    "        return max(num_clients, self.min_evaluate_clients), self.min_available_clients\n",
    "    \n",
    "   \n",
    "    # def initialize_parameters(\n",
    "    #     self, client_manager: ClientManager\n",
    "    # ) -> Optional[Parameters]:\n",
    "    #     \"\"\"Initialize global model parameters.\"\"\"\n",
    "    #     net = Net()\n",
    "    #     ndarrays = get_parameters(net)\n",
    "    #     return ndarrays_to_parameters(ndarrays)\n",
    "    def initialize_parameters(\n",
    "        self, client_manager: ClientManager) -> Optional[Parameters]:\n",
    "        \"\"\"Initialize global model parameters.\"\"\"\n",
    "        print(\"Initializing global model parameters\")\n",
    "        net = Net().to(DEVICE)  # Make sure to initialize on the correct device\n",
    "        ndarrays = get_parameters(net)\n",
    "        \n",
    "        # Debug: print summary of parameters to verify they're not all zeros or random\n",
    "        for i, param in enumerate(ndarrays):\n",
    "            print(f\"Global init param {i}: shape={param.shape}, mean={param.mean():.6f}, std={param.std():.6f}\")\n",
    "        \n",
    "        return ndarrays_to_parameters(ndarrays) \n",
    "\n",
    "    def evaluate(\n",
    "        self, server_round: int, parameters: Parameters\n",
    "    ) -> Optional[tuple[float, dict[str, Scalar]]]:\n",
    "        \"\"\"Evaluate model parameters using an evaluation function.\"\"\"\n",
    "        if self.evaluate_fn is None:\n",
    "            # No evaluation function provided\n",
    "            return None\n",
    "        parameters_ndarrays = parameters_to_ndarrays(parameters)\n",
    "        eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})\n",
    "        if eval_res is None:\n",
    "            return None\n",
    "        loss, metrics = eval_res\n",
    "\n",
    "        if server_round in fed_avg_model_results:\n",
    "            expand_fed_avg_result= {**fed_avg_model_results[server_round], \"global_loss\": loss, \"global_metrics\": metrics}\n",
    "        else:\n",
    "            expand_fed_avg_result= {\"global_loss\": loss, \"global_metrics\": metrics}\n",
    "\n",
    "        fed_avg_model_results[server_round] = expand_fed_avg_result\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "    def configure_fit(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, FitIns]]:\n",
    "        \"\"\"Configure the next round of training.\"\"\"\n",
    "        \n",
    "        config = {}\n",
    "        \n",
    "        sample_size, min_num_clients = self.num_fit_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "        \n",
    "        fit_configurations = []\n",
    "        for idx, client in enumerate(clients):\n",
    "            fit_configurations.append((client, FitIns(parameters, config)))\n",
    "\n",
    "        \n",
    "        return fit_configurations\n",
    "    \n",
    "\n",
    "    def configure_evaluate(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, EvaluateIns]]:\n",
    "        \"\"\"Configure the next round of evaluation.\"\"\"\n",
    "        if self.fraction_evaluate == 0.0:\n",
    "            return []\n",
    "        config = {}\n",
    "        evaluate_ins = EvaluateIns(parameters, config)\n",
    "\n",
    "        # Sample clients\n",
    "        sample_size, min_num_clients = self.num_evaluation_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "\n",
    "        # Return client/config pairs\n",
    "        return [(client, evaluate_ins) for client in clients]\n",
    "\n",
    "# TEMPORARY \n",
    "    # def aggregate_fit(\n",
    "    #     self,\n",
    "    #     server_round: int,\n",
    "    #     results: List[Tuple[ClientProxy, FitRes]],\n",
    "    #     failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    # ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "    #     \"\"\"Aggregate fit results using weighted average.\"\"\"\n",
    "    #     print(f\"\\n==== Aggregating parameters for round {server_round} ====\")\n",
    "    #     print(f\"  Number of client results: {len(results)}\")\n",
    "        \n",
    "    #     # get size of parameters in bytes\n",
    "    #     total_size = 0\n",
    "    #     for client, fit_res in results:\n",
    "    #         total_size += get_parameters_size(fit_res.parameters) * 2\n",
    "        \n",
    "    #     if server_round in fed_avg_result:\n",
    "    #         expand_fed_avg_result = {**fed_avg_result[server_round], \"total_size\": total_size}\n",
    "    #     else:\n",
    "    #         expand_fed_avg_result = {\"total_size\": total_size}\n",
    "        \n",
    "    #     fed_avg_result[server_round] = expand_fed_avg_result\n",
    "        \n",
    "    #     # Check client results for significant differences\n",
    "    #     if len(results) >= 2:\n",
    "    #         client_params = [parameters_to_ndarrays(fit_res.parameters) for _, fit_res in results]\n",
    "            \n",
    "    #         # Compare first two clients' parameters\n",
    "    #         diff_sum = 0\n",
    "    #         for i, (param1, param2) in enumerate(zip(client_params[0], client_params[1])):\n",
    "    #             diff = np.abs(param1 - param2).mean()\n",
    "    #             diff_sum += diff\n",
    "    #             if i < 5:  # Print first few layers only to avoid too much output\n",
    "    #                 print(f\"  Layer {i} diff between clients: {diff:.6f}\")\n",
    "            \n",
    "    #         print(f\"  Average parameter difference between clients: {diff_sum/len(client_params[0]):.6f}\")\n",
    "        \n",
    "    #     weights_results = [\n",
    "    #         (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)\n",
    "    #         for _, fit_res in results\n",
    "    #     ]\n",
    "        \n",
    "    #     parameters_aggregated = ndarrays_to_parameters(aggregate(weights_results))\n",
    "    #     metrics_aggregated = {}\n",
    "        \n",
    "    #     return parameters_aggregated, metrics_aggregated\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate fit results using weighted average.\"\"\"\n",
    "\n",
    "        # get size of parameters in bytes\n",
    "        total_size = 0\n",
    "        for client, fit_res in results:\n",
    "            total_size += get_parameters_size(fit_res.parameters) *2\n",
    "        \n",
    "\n",
    "        if server_round in fed_avg_result:\n",
    "            expand_fed_avg_result= {**fed_avg_result[server_round], \"total_size\": total_size}\n",
    "        else:\n",
    "            expand_fed_avg_result= {\"total_size\": total_size}\n",
    "\n",
    "        fed_avg_result[server_round] = expand_fed_avg_result\n",
    "\n",
    "\n",
    "        weights_results = [\n",
    "            (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)\n",
    "            for _, fit_res in results\n",
    "        ]\n",
    "        parameters_aggregated = ndarrays_to_parameters(aggregate(weights_results))\n",
    "        metrics_aggregated = {}\n",
    "        return parameters_aggregated, metrics_aggregated\n",
    "\n",
    "\n",
    "    def aggregate_evaluate(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, EvaluateRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
    "    ) -> Tuple[Optional[float], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate evaluation losses using weighted average.\"\"\"\n",
    "\n",
    "        if not results:\n",
    "            return None, {}\n",
    "\n",
    "        total_loss = 0\n",
    "        for _, evaluate_res in results:\n",
    "            total_loss += evaluate_res.loss \n",
    "\n",
    "\n",
    "        if server_round in fed_avg_result:\n",
    "            expand_fed_avg_result= {**fed_avg_result[server_round], \"total_loss\": total_loss}\n",
    "        else:\n",
    "            expand_fed_avg_result= {\"total_loss\": total_loss}\n",
    "\n",
    "        fed_avg_result[server_round] = expand_fed_avg_result\n",
    "\n",
    "        loss_aggregated = weighted_loss_avg(\n",
    "            [\n",
    "                (evaluate_res.num_examples, evaluate_res.loss)\n",
    "                for _, evaluate_res in results\n",
    "            ]\n",
    "        )\n",
    "        metrics_aggregated = {}\n",
    "        return loss_aggregated, metrics_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalFlowerClient(NumPyClient):\n",
    "    def __init__(self, partition_id, net, trainloader, valloader):\n",
    "        self.partition_id = partition_id\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.partition_id}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, epochs=EPOCHS)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "\n",
    "def client_fn(context: Context) -> Client:\n",
    "    net = Net().to(DEVICE)\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "    trainloader, valloader, _ = load_datasets(partition_id, num_partitions)\n",
    "    return NormalFlowerClient(partition_id, net, trainloader, valloader).to_client()\n",
    "\n",
    "\n",
    "# Create the ClientApp\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=20, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing global model parameters\n",
      "Global init param 0: shape=(32, 3, 3, 3), mean=0.001680, std=0.109916\n",
      "Global init param 1: shape=(32,), mean=0.015101, std=0.091035\n",
      "Global init param 2: shape=(64, 32, 3, 3), mean=0.000179, std=0.033973\n",
      "Global init param 3: shape=(64,), mean=0.000457, std=0.033434\n",
      "Global init param 4: shape=(128, 64, 3, 3), mean=-0.000065, std=0.024094\n",
      "Global init param 5: shape=(128,), mean=0.001161, std=0.022244\n",
      "Global init param 6: shape=(128, 128, 3, 3), mean=0.000013, std=0.017005\n",
      "Global init param 7: shape=(128,), mean=0.000738, std=0.016256\n",
      "Global init param 8: shape=(256, 128, 3, 3), mean=0.000038, std=0.017021\n",
      "Global init param 9: shape=(256,), mean=-0.000465, std=0.017100\n",
      "Global init param 10: shape=(256, 256, 3, 3), mean=-0.000007, std=0.012010\n",
      "Global init param 11: shape=(256,), mean=-0.000898, std=0.011698\n",
      "Global init param 12: shape=(1024, 4096), mean=-0.000001, std=0.009021\n",
      "Global init param 13: shape=(1024,), mean=-0.000138, std=0.009091\n",
      "Global init param 14: shape=(512, 1024), mean=0.000004, std=0.018052\n",
      "Global init param 15: shape=(512,), mean=0.000446, std=0.018305\n",
      "Global init param 16: shape=(10, 512), mean=0.000108, std=0.025627\n",
      "Global init param 17: shape=(10,), mean=-0.004397, std=0.021421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 0.07207898287773132, {'accuracy': 0.1}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m [Client 1] fit, config: {}\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 1: train loss 0.06182160601019859, accuracy 0.24735618390459763\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m [Client 2] fit, config: {}\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 1: train loss 0.061630524694919586, accuracy 0.25890647266181654\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 1: train loss 0.059897202998399734, accuracy 0.26897689768976896\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 2: train loss 0.048112839460372925, accuracy 0.4257106427660691\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 2: train loss 0.0482134148478508, accuracy 0.42244224422442245\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 4: train loss 0.03479151427745819, accuracy 0.5946898672466812\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 3: train loss 0.042165357619524, accuracy 0.5000750075007501\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 3: train loss 0.04217009246349335, accuracy 0.5031125778144454\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 4: train loss 0.036715809255838394, accuracy 0.568106810681068\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 4: train loss 0.03663581237196922, accuracy 0.5745143628590714\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 5: train loss 0.0323873870074749, accuracy 0.6226372637263726\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 8: train loss 0.015842176973819733, accuracy 0.8187204680117003\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 5: train loss 0.03172658383846283, accuracy 0.6319657991449786\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 6: train loss 0.027326885610818863, accuracy 0.6792169804245106\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 7: train loss 0.022430460900068283, accuracy 0.7403435085877147\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 3 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 8: train loss 0.01780889928340912, accuracy 0.7965949148728718\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      fit progress: (1, 0.07378268280029297, {'accuracy': 0.1}, 171.669287209)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m [Client 2] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 3 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m [Client 2] fit, config: {}\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m [Client 0] evaluate, config: {}\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m [Client 0] fit, config: {}\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 1: train loss 0.051892612129449844, accuracy 0.36998424960624016\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 1: train loss 0.05387675389647484, accuracy 0.3304582614565364\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 2: train loss 0.041266802698373795, accuracy 0.5133513351335134\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 4: train loss 0.025801043957471848, accuracy 0.7034425860646516\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 3: train loss 0.03549189120531082, accuracy 0.5880897022425561\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 3: train loss 0.035242196172475815, accuracy 0.5862586258625863\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 6: train loss 0.01670498587191105, accuracy 0.8070951773794345\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 7: train loss 0.013148332014679909, accuracy 0.8514212855321383\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 5: train loss 0.02677571214735508, accuracy 0.6940673516837921\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 8: train loss 0.010109448805451393, accuracy 0.8822470561764044\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 6: train loss 0.023397207260131836, accuracy 0.7344183604590114\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 6: train loss 0.023141827434301376, accuracy 0.7321482148214822\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 7: train loss 0.0201399102807045, accuracy 0.7719942998574965\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 7: train loss 0.019598085433244705, accuracy 0.774977497749775\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 8: train loss 0.017128286883234978, accuracy 0.8070951773794345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (2, 0.062079529559612275, {'accuracy': 0.1947}, 337.198503125)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m [Client 0] evaluate, config: {}\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 8: train loss 0.016457198187708855, accuracy 0.8127062706270627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 3 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m [Client 0] fit, config: {}\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m [Client 2] evaluate, config: {}\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 1: train loss 0.0351703055202961, accuracy 0.5948398709967749\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m [Client 2] fit, config: {}\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 1: train loss 0.03654003143310547, accuracy 0.5768394209855247\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 1: train loss 0.03537522628903389, accuracy 0.59000900090009\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 2: train loss 0.02769852615892887, accuracy 0.6834170854271356\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 2: train loss 0.026239488273859024, accuracy 0.7020702070207021\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 4: train loss 0.017344394698739052, accuracy 0.800120003000075\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 3: train loss 0.02270754612982273, accuracy 0.7417685442136054\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 3: train loss 0.021538127213716507, accuracy 0.7535253525352535\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 5: train loss 0.0135935815051198, accuracy 0.8463211580289507\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 4: train loss 0.01847204379737377, accuracy 0.789169729243231\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 6: train loss 0.011111400090157986, accuracy 0.874596864921623\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 4: train loss 0.017168039456009865, accuracy 0.8054305430543054\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 5: train loss 0.014817957766354084, accuracy 0.8306457661441536\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 8: train loss 0.007623172365128994, accuracy 0.9150978774469362\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 6: train loss 0.011061258614063263, accuracy 0.8755625562556255\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 7: train loss 0.008091678842902184, accuracy 0.9102910291029103\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 3 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 8: train loss 0.007164564449340105, accuracy 0.9217671767176717\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      fit progress: (3, 0.029222033578157426, {'accuracy': 0.7013}, 502.02360325)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m [Client 2] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 3 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m [Client 1] fit, config: {}\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m [Client 0] evaluate, config: {}\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 1: train loss 0.021251721307635307, accuracy 0.761026102610261\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m [Client 0] fit, config: {}\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 1: train loss 0.020804546773433685, accuracy 0.766219155478887\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26351)\u001b[0m Epoch 2: train loss 0.014212925918400288, accuracy 0.8389709742743569\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 3: train loss 0.010091420263051987, accuracy 0.8883888388838884\n",
      "\u001b[36m(ClientAppActor pid=26352)\u001b[0m Epoch 2: train loss 0.014256074093282223, accuracy 0.8392709817745444\n",
      "\u001b[36m(ClientAppActor pid=26353)\u001b[0m Epoch 4: train loss 0.007475954480469227, accuracy 0.9160666066606661\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "net = Net().to(DEVICE)\n",
    "\n",
    "_, _, testloader = load_datasets(0, NUM_PARTITIONS)\n",
    "\n",
    "evaluate_fn = get_evaluate_fn(testloader, net)\n",
    "\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Configure the server for just 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=NUM_OF_ROUNDS)\n",
    "    return ServerAppComponents(\n",
    "        config=config,\n",
    "        strategy=ModifiedFedAvg(\n",
    "            evaluate_fn=evaluate_fn\n",
    "        ),\n",
    "    )\n",
    "\n",
    "server = ServerApp(server_fn=server_fn)\n",
    "\n",
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_PARTITIONS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "with open(f'results/fed_avg_results.p', 'wb') as file:\n",
    "    pickle.dump(fed_avg_result, file)\n",
    "\n",
    "with open(f'results/fed_avg_model_results.p', 'wb') as file:\n",
    "    pickle.dump(fed_avg_model_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# fed_avg_rounds = list(fed_avg_result.keys())\n",
    "# fed_avg_sizes = [fed_avg_result[round][\"total_size\"] for round in fed_avg_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_avg_rounds, fed_avg_sizes, marker='o', linestyle='-', color='b')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Total Size of Parameters (bytes)')\n",
    "# plt.title('Total Size of Parameters for Each Round')\n",
    "# plt.grid(True)\n",
    "\n",
    "# fed_avg_losses = [fed_avg_result[round][\"total_loss\"] for round in fed_avg_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_avg_rounds, fed_avg_losses, marker='o', linestyle='-', color='b')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Total Loss')\n",
    "# plt.title('Total Loss for Each Round')\n",
    "# plt.grid(True)\n",
    "\n",
    "# fed_avg_model_rounds = list(fed_avg_model_results.keys())\n",
    "\n",
    "# fed_avg_accuracies = [fed_avg_model_results[round][\"global_metrics\"][\"accuracy\"] for round in fed_avg_model_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_accuracies, marker='o', linestyle='-', color='b')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Accuracy for Each Round')\n",
    "# plt.grid(True)\n",
    "\n",
    "# fed_avg_global_losses = [fed_avg_model_results[round][\"global_loss\"] for round in fed_avg_model_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_global_losses, marker='o', linestyle='-', color='b')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Loss for Each Round')\n",
    "# plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedProx experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedProxFlowerClient(NumPyClient):\n",
    "    def __init__(self, partition_id, net, trainloader, valloader):\n",
    "        self.partition_id = partition_id\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.partition_id}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        global_params = copy.deepcopy(self.net).parameters()\n",
    "        proxima_train(self.net, self.trainloader, EPOCHS, config[\"proximal_mu\"], global_params)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "def client_fn(context: Context) -> Client:\n",
    "    net = Net().to(DEVICE)\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "    trainloader, valloader, _ = load_datasets(partition_id, num_partitions)\n",
    "    return FedProxFlowerClient(partition_id, net, trainloader, valloader).to_client()\n",
    "\n",
    "\n",
    "# Create the ClientApp\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_prox_result = {}\n",
    "\n",
    "fed_prox_model_results = {}\n",
    "\n",
    "class ModifiedFedProx(ModifiedFedAvg):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        fraction_fit: float = 1.0,\n",
    "        fraction_evaluate: float = 1.0,\n",
    "        min_fit_clients: int = 2,\n",
    "        min_evaluate_clients: int = 2,\n",
    "        min_available_clients: int = 2,\n",
    "        evaluate_fn: Optional[\n",
    "            Callable[\n",
    "                [int, NDArrays, dict[str, Scalar]],\n",
    "                Optional[tuple[float, dict[str, Scalar]]],\n",
    "            ]\n",
    "        ] = None,\n",
    "        on_fit_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,\n",
    "        on_evaluate_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,\n",
    "        accept_failures: bool = True,\n",
    "        initial_parameters: Optional[Parameters] = None,\n",
    "        fit_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
    "        evaluate_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
    "        proximal_mu: float,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            fraction_fit=fraction_fit,\n",
    "            fraction_evaluate=fraction_evaluate,\n",
    "            min_fit_clients=min_fit_clients,\n",
    "            min_evaluate_clients=min_evaluate_clients,\n",
    "            min_available_clients=min_available_clients,\n",
    "            evaluate_fn=evaluate_fn,\n",
    "            on_fit_config_fn=on_fit_config_fn,\n",
    "            on_evaluate_config_fn=on_evaluate_config_fn,\n",
    "            accept_failures=accept_failures,\n",
    "            initial_parameters=initial_parameters,\n",
    "            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n",
    "            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n",
    "        )\n",
    "        self.proximal_mu = proximal_mu\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"ModifiedFedProx\"\n",
    "    \n",
    "\n",
    "    def configure_fit(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> list[tuple[ClientProxy, FitIns]]:\n",
    "        \"\"\"Configure the next round of training.\n",
    "\n",
    "        Sends the proximal factor mu to the clients\n",
    "        \"\"\"\n",
    "        # Get the standard client/config pairs from the FedAvg super-class\n",
    "        client_config_pairs = super().configure_fit(\n",
    "            server_round, parameters, client_manager\n",
    "        )\n",
    "\n",
    "        # Return client/config pairs with the proximal factor mu added\n",
    "        return [\n",
    "            (\n",
    "                client,\n",
    "                FitIns(\n",
    "                    fit_ins.parameters,\n",
    "                    {**fit_ins.config, \"proximal_mu\": self.proximal_mu},\n",
    "                ),\n",
    "            )\n",
    "            for client, fit_ins in client_config_pairs\n",
    "        ]\n",
    "    \n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate fit results using weighted average.\"\"\"\n",
    "        \n",
    "        total_size = 0\n",
    "        for client, fit_res in results:\n",
    "            total_size += get_parameters_size(fit_res.parameters) *2\n",
    "        print(f\"total size: {total_size}\")\n",
    "        \n",
    "        if fed_prox_result.get(server_round):\n",
    "            fed_prox_result[server_round][\"total_size\"] = total_size\n",
    "        else:\n",
    "            fed_prox_result[server_round] = {\"total_size\": total_size}\n",
    "        \n",
    "\n",
    "        weights_results = [\n",
    "            (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)\n",
    "            for _, fit_res in results\n",
    "        ]\n",
    "\n",
    "        parameters_aggregated = ndarrays_to_parameters(aggregate(weights_results))\n",
    "        metrics_aggregated = {}\n",
    "        return parameters_aggregated, metrics_aggregated\n",
    "\n",
    "\n",
    "    def aggregate_evaluate(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, EvaluateRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
    "    ) -> Tuple[Optional[float], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate evaluation losses using weighted average.\"\"\"\n",
    "\n",
    "        if not results:\n",
    "            return None, {}\n",
    "        \n",
    "        total_loss = 0\n",
    "        for _, evaluate_res in results:\n",
    "            total_loss += evaluate_res.loss\n",
    "\n",
    "        if fed_prox_result.get(server_round):\n",
    "            fed_prox_result[server_round][\"total_loss\"] = total_loss\n",
    "        else:\n",
    "            fed_prox_result[server_round] = {\"total_loss\": total_loss}\n",
    "\n",
    "        loss_aggregated = weighted_loss_avg(\n",
    "            [\n",
    "                (evaluate_res.num_examples, evaluate_res.loss)\n",
    "                for _, evaluate_res in results\n",
    "            ]\n",
    "        )\n",
    "        metrics_aggregated = {}\n",
    "        return loss_aggregated, metrics_aggregated\n",
    "    \n",
    "\n",
    "    def evaluate(\n",
    "        self, server_round: int, parameters: Parameters\n",
    "    ) -> Optional[tuple[float, dict[str, Scalar]]]:\n",
    "        \"\"\"Evaluate model parameters using an evaluation function.\"\"\"\n",
    "        if self.evaluate_fn is None:\n",
    "            # No evaluation function provided\n",
    "            return None\n",
    "        parameters_ndarrays = parameters_to_ndarrays(parameters)\n",
    "        eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})\n",
    "        if eval_res is None:\n",
    "            return None\n",
    "        \n",
    "        if server_round in fed_prox_model_results:  \n",
    "            expand_fed_prox_model_results= {**fed_prox_model_results[server_round], \"global_loss\": eval_res[0], \"global_metrics\": eval_res[1]}\n",
    "        else:\n",
    "            expand_fed_prox_model_results= {\"global_loss\": eval_res[0], \"global_metrics\": eval_res[1]}\n",
    "        \n",
    "        fed_prox_model_results[server_round] = expand_fed_prox_model_results\n",
    "        \n",
    "        loss, metrics = eval_res\n",
    "        return loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(DEVICE)\n",
    "\n",
    "_, _, testloader = load_datasets(0, NUM_PARTITIONS)\n",
    "\n",
    "evaluate_fn = get_evaluate_fn(testloader, net)\n",
    "\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Configure the server for just 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=NUM_OF_ROUNDS)\n",
    "    return ServerAppComponents(\n",
    "        config=config,\n",
    "        strategy=ModifiedFedProx(proximal_mu=0.1, evaluate_fn=evaluate_fn),\n",
    "    )\n",
    "\n",
    "server = ServerApp(server_fn=server_fn)\n",
    "\n",
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_PARTITIONS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results/fed_prox_result.p', 'wb') as file:\n",
    "    pickle.dump(fed_prox_result, file)\n",
    "\n",
    "with open(f'results/fed_prox_model_results.p', 'wb') as file:\n",
    "    pickle.dump(fed_prox_model_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_prox_rounds = list(fed_prox_result.keys())\n",
    "fed_prox_sizes = [fed_prox_result[round][\"total_size\"] for round in fed_prox_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_prox_rounds, fed_prox_sizes, marker='o', linestyle='-', color='b', label='FedProx')\n",
    "# plt.plot(fed_part_avg_rounds, fed_part_avg_sizes, marker='o', linestyle='-', color='r', label='FedPartAvg')\n",
    "# plt.plot(fed_avg_rounds, fed_avg_sizes, marker='o', linestyle='-', color='g', label='FedAvg')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Total Size of Parameters (bytes)')\n",
    "# plt.title('Total Size of Parameters for Each Round')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# fed_prox_losses = [fed_prox_result[round][\"total_loss\"] for round in fed_prox_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_prox_rounds, fed_prox_losses, marker='o', linestyle='-', color='b', label='FedProx')\n",
    "# plt.plot(fed_part_avg_rounds, fed_part_avg_losses, marker='o', linestyle='-', color='r', label='FedPartAvg')\n",
    "# plt.plot(fed_avg_rounds, fed_avg_losses, marker='o', linestyle='-', color='g', label='FedAvg')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Total Loss')\n",
    "# plt.title('Total Loss for Each Round')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "\n",
    "# fed_prox_model_rounds = list(fed_prox_model_results.keys())\n",
    "# fed_prox_accuracies = [fed_prox_model_results[round][\"global_metrics\"][\"accuracy\"] for round in fed_prox_model_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# # plt.plot(fed_part_prox_model_rounds, fed_part_prox_accuracies, marker='o', linestyle='-', color='b', label='FedPartProx')\n",
    "# plt.plot(fed_part_avg_model_rounds, fed_part_avg_accuracies, marker='o', linestyle='-', color='r', label='FedPartAvg')\n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_accuracies, marker='o', linestyle='-', color='g', label='FedAvg')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Accuracy for Each Round')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# fed_prox_global_losses = [fed_prox_model_results[round][\"global_loss\"] for round in fed_prox_model_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# # plt.plot(fed_part_prox_model_rounds, fed_part_prox_global_losses, marker='o', linestyle='-', color='b', label='FedPartProx')\n",
    "# plt.plot(fed_part_avg_model_rounds, fed_part_avg_global_losses, marker='o', linestyle='-', color='r', label='FedPartAvg')   \n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_global_losses, marker='o', linestyle='-', color='g', label='FedAvg')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Loss for Each Round')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedMoon experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "class FedMoonNoFreezeFlowerClient(NumPyClient):\n",
    "    def __init__(self, partition_id, net, trainloader, valloader):\n",
    "        self.partition_id = partition_id\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.model_dir = \"models\"\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.partition_id}] get_parameters\")\n",
    "        parameters = get_parameters(self.net)\n",
    "        trainable_layer = config[\"trainable_layers\"]\n",
    "        self._save_model_state()\n",
    "        \n",
    "        if trainable_layer == -1:\n",
    "            return parameters\n",
    "        \n",
    "        trained_layer = [parameters[trainable_layer*2], parameters[trainable_layer*2 +1]]\n",
    "        return trained_layer\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] fit, config: {config}\")\n",
    "\n",
    "        # load previous model\n",
    "        if not os.path.exists(os.path.join(self.model_dir, str(self.partition_id))):\n",
    "            prev_model = copy.deepcopy(self.net)\n",
    "        else:\n",
    "            # initialise and load params from model_dir\n",
    "            prev_model = type(self.net)() \n",
    "            prev_model.load_state_dict(\n",
    "                torch.load(\n",
    "                    os.path.join(self.model_dir, str(self.partition_id), \"prev_net.pt\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # update params for current model (loading global params)\n",
    "        set_parameters(self.net, parameters)\n",
    "\n",
    "        # create global model (same params that were just loaded)\n",
    "        global_model = type(self.net)()\n",
    "        global_model.load_state_dict(self.net.state_dict())\n",
    "        global_model.to(DEVICE)\n",
    "        \n",
    "        train_moon(self.net, self.trainloader, global_model, prev_model, EPOCHS, 5, 0.5)\n",
    "\n",
    "        # save current model \n",
    "        if not os.path.exists(os.path.join(self.model_dir, str(self.partition_id))):\n",
    "            os.makedirs(os.path.join(self.model_dir, str(self.partition_id)))\n",
    "        torch.save(\n",
    "            self.net.state_dict(),\n",
    "            os.path.join(self.model_dir, str(self.partition_id), \"prev_net.pt\"),\n",
    "        )\n",
    "\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test_moon(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "def client_fn(context: Context) -> Client:\n",
    "    net = MoonNet().to(DEVICE)\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "    trainloader, valloader, _ = load_datasets(partition_id, num_partitions)\n",
    "    return FedMoonNoFreezeFlowerClient(partition_id, net, trainloader, valloader).to_client()\n",
    "\n",
    "\n",
    "# Create the ClientApp\n",
    "client = ClientApp(client_fn=client_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import sys\n",
    "\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.strategy.aggregate import aggregate, weighted_loss_avg\n",
    "\n",
    "def get_parameters_size(params: Parameters) -> int:\n",
    "    size = sys.getsizeof(params)  # Base size of the dataclass instance\n",
    "    size += sys.getsizeof(params.tensor_type)  # Size of the string\n",
    "    size += sys.getsizeof(params.tensors)  # Size of the list container\n",
    "    size += sum(sys.getsizeof(tensor) for tensor in params.tensors)  # Size of each bytes object\n",
    "    return size\n",
    "\n",
    "fed_moon_no_freeze_result = {}\n",
    "fed_moon_model_no_freeze_results = {}\n",
    "\n",
    "# basically same as normal FedAvg, just added freezing and modified result dict names\n",
    "class FedMoonNoFreeze(Strategy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fraction_fit: float = 1.0,\n",
    "        fraction_evaluate: float = 1.0,\n",
    "        min_fit_clients: int = 2,\n",
    "        min_evaluate_clients: int = 2,\n",
    "        min_available_clients: int = 2,\n",
    "        evaluate_fn: Optional[\n",
    "            Callable[\n",
    "                [int, NDArrays, dict[str, Scalar]],\n",
    "                Optional[tuple[float, dict[str, Scalar]]],\n",
    "            ]\n",
    "        ] = None,\n",
    "        on_fit_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,\n",
    "        on_evaluate_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,\n",
    "        accept_failures: bool = True,\n",
    "        initial_parameters: Optional[Parameters] = None,\n",
    "        fit_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
    "        evaluate_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
    "        inplace: bool = True,\n",
    "        layer_update_strategy: str = \"sequential\",\n",
    "        \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.fraction_fit = fraction_fit\n",
    "        self.fraction_evaluate = fraction_evaluate\n",
    "        self.min_fit_clients = min_fit_clients\n",
    "        self.min_evaluate_clients = min_evaluate_clients\n",
    "        self.min_available_clients = min_available_clients\n",
    "        self.evaluate_fn = evaluate_fn\n",
    "        self.on_fit_config_fn = on_fit_config_fn\n",
    "        self.on_evaluate_config_fn = on_evaluate_config_fn\n",
    "        self.accept_failures = accept_failures\n",
    "        self.initial_parameters = initial_parameters\n",
    "        self.fit_metrics_aggregation_fn = fit_metrics_aggregation_fn\n",
    "        self.evaluate_metrics_aggregation_fn = evaluate_metrics_aggregation_fn\n",
    "        self.inplace = inplace\n",
    "        self.layer_training_sequence = []\n",
    "        self.training_sequence_index = 0\n",
    "        self.latest_parameters = initial_parameters\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"FedMoon\"\n",
    "    \n",
    "    def num_fit_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Return sample size and required number of clients.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_fit)\n",
    "        return max(num_clients, self.min_fit_clients), self.min_available_clients\n",
    "\n",
    "    def num_evaluation_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Use a fraction of available clients for evaluation.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_evaluate)\n",
    "        return max(num_clients, self.min_evaluate_clients), self.min_available_clients\n",
    "   \n",
    "    def initialize_parameters(\n",
    "        self, client_manager: ClientManager\n",
    "    ) -> Optional[Parameters]:\n",
    "        \"\"\"Initialize global model parameters.\"\"\"\n",
    "        net = Net()\n",
    "        ndarrays = get_parameters(net)\n",
    "        return ndarrays_to_parameters(ndarrays)\n",
    "\n",
    "    def evaluate(\n",
    "        self, server_round: int, parameters: Parameters\n",
    "    ) -> Optional[tuple[float, dict[str, Scalar]]]:\n",
    "        \"\"\"Evaluate model parameters using an evaluation function.\"\"\"\n",
    "        if self.evaluate_fn is None:\n",
    "            # No evaluation function provided\n",
    "            return None\n",
    "        parameters_ndarrays = parameters_to_ndarrays(parameters)\n",
    "        eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})\n",
    "        if eval_res is None:\n",
    "            return None\n",
    "        loss, metrics = eval_res\n",
    "\n",
    "        if server_round in fed_moon_model_no_freeze_results:\n",
    "            expand_fed_moon_no_freeze_result= {**fed_moon_model_no_freeze_results[server_round], \"global_loss\": loss, \"global_metrics\": metrics}\n",
    "        else:\n",
    "            expand_fed_moon_no_freeze_result= {\"global_loss\": loss, \"global_metrics\": metrics}\n",
    "\n",
    "        fed_moon_model_no_freeze_results[server_round] = expand_fed_moon_no_freeze_result\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "\n",
    "    def configure_fit(\n",
    "        # includes layer freezing\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, FitIns]]:\n",
    "        \"\"\"Configure the next round of training.\"\"\"\n",
    "        config = {}\n",
    "        \n",
    "        sample_size, min_num_clients = self.num_fit_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "        \n",
    "        fit_configurations = []\n",
    "        for idx, client in enumerate(clients):\n",
    "            fit_configurations.append((client, FitIns(parameters, config)))\n",
    "\n",
    "        self.training_sequence_index = self.training_sequence_index + 1\n",
    "        \n",
    "        return fit_configurations\n",
    "    \n",
    "    def configure_evaluate(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, EvaluateIns]]:\n",
    "        \"\"\"Configure the next round of evaluation.\"\"\"\n",
    "        if self.fraction_evaluate == 0.0:\n",
    "            return []\n",
    "        config = {}\n",
    "        evaluate_ins = EvaluateIns(parameters, config)\n",
    "\n",
    "        # Sample clients\n",
    "        sample_size, min_num_clients = self.num_evaluation_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "\n",
    "        # Return client/config pairs\n",
    "        return [(client, evaluate_ins) for client in clients]\n",
    "\n",
    "\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate fit results using weighted average.\"\"\"\n",
    "\n",
    "        # get size of parameters in bytes\n",
    "        total_size = 0\n",
    "        for client, fit_res in results:\n",
    "            total_size += get_parameters_size(fit_res.parameters) * 2\n",
    "        \n",
    "        if server_round in fed_moon_no_freeze_result:\n",
    "            expand_fed_moon_no_freeze_result= {**fed_moon_no_freeze_result[server_round], \"total_size\": total_size}\n",
    "        else:\n",
    "            expand_fed_moon_no_freeze_result= {\"total_size\": total_size}\n",
    "\n",
    "        fed_moon_no_freeze_result[server_round] = expand_fed_moon_no_freeze_result\n",
    "\n",
    "        weights_results = [\n",
    "            (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)\n",
    "            for _, fit_res in results\n",
    "        ]\n",
    "        \n",
    "        aggregated_weights = aggregate(weights_results)\n",
    "        \n",
    "        self.latest_parameters = ndarrays_to_parameters(aggregated_weights)\n",
    "\n",
    "        metrics_aggregated = {}\n",
    "        return self.latest_parameters, metrics_aggregated\n",
    "\n",
    "    \n",
    "\n",
    "    def aggregate_evaluate(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, EvaluateRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
    "    ) -> Tuple[Optional[float], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate evaluation losses using weighted average.\"\"\"\n",
    "\n",
    "        if not results:\n",
    "            return None, {}\n",
    "\n",
    "        total_loss = 0\n",
    "        for _, evaluate_res in results:\n",
    "            total_loss += evaluate_res.loss \n",
    "\n",
    "\n",
    "        if server_round in fed_moon_no_freeze_result:\n",
    "            expand_fed_moon_no_freeze_result= {**fed_moon_no_freeze_result[server_round], \"total_loss\": total_loss}\n",
    "        else:\n",
    "            expand_fed_moon_no_freeze_result= {\"total_loss\": total_loss}\n",
    "\n",
    "        fed_moon_no_freeze_result[server_round] = expand_fed_moon_no_freeze_result\n",
    "\n",
    "        loss_aggregated = weighted_loss_avg(\n",
    "            [\n",
    "                (evaluate_res.num_examples, evaluate_res.loss)\n",
    "                for _, evaluate_res in results\n",
    "            ]\n",
    "        )\n",
    "        metrics_aggregated = {}\n",
    "        return loss_aggregated, metrics_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train FedMOON\n",
    "\n",
    "\n",
    "_, _, testloader = load_datasets(0, NUM_PARTITIONS)\n",
    "net = MoonNet().to(DEVICE)\n",
    "evaluate_fn = get_evaluate_fn_moon(testloader, net)\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Configure the server for just 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=NUM_OF_ROUNDS)\n",
    "    return ServerAppComponents(\n",
    "        config=config,\n",
    "        strategy=FedMoonNoFreeze(\n",
    "            evaluate_fn=evaluate_fn\n",
    "        )\n",
    "    )\n",
    "\n",
    "server = ServerApp(server_fn=server_fn)\n",
    "\n",
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_PARTITIONS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results/fed_moon_no_freeze_result.p', 'wb') as file:\n",
    "    pickle.dump(fed_moon_no_freeze_result, file)\n",
    "\n",
    "with open(f'results/fed_moon_model_no_freeze_results.p', 'wb') as file:\n",
    "    pickle.dump(fed_moon_model_no_freeze_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fed_moon_rounds = list(fed_moon_no_freeze_result.keys())\n",
    "# fed_moon_sizes = [fed_moon_no_freeze_result[round][\"total_size\"] for round in fed_moon_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_avg_rounds, fed_avg_sizes, marker='o', linestyle='-', color='b', label='FedAvg')\n",
    "# plt.plot(fed_part_avg_rounds, fed_part_avg_sizes, marker='o', linestyle='-', color='r', label='FedPartAvg')\n",
    "# plt.plot(fed_prox_rounds, fed_prox_sizes, marker='o', linestyle='-', color='g', label='FedProx')\n",
    "# plt.plot(fed_part_prox_rounds, fed_part_prox_sizes, marker='o', linestyle='-', color='y', label='FedPartProx')\n",
    "# plt.plot(fed_moon_rounds, fed_moon_sizes, marker='o', linestyle='-', color='c', label='FedMoon')\n",
    "# plt.plot(fed_part_moon_rounds, fed_part_moon_sizes, marker='o', linestyle='-', color='purple', label='FedPartMoon')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Communication Cost (bytes)')\n",
    "# plt.title('Communication Cost for Each Round')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# fed_moon_losses = [fed_moon_no_freeze_result[round][\"total_loss\"] for round in fed_moon_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_avg_rounds, fed_avg_losses, marker='o', linestyle='-', color='b', label='FedAvg')\n",
    "# plt.plot(fed_part_avg_rounds, fed_part_avg_losses, marker='o', linestyle='-', color='r', label='FedPartAvg')\n",
    "# plt.plot(fed_prox_rounds, fed_prox_losses, marker='o', linestyle='-', color='g', label='FedProx')\n",
    "# plt.plot(fed_part_prox_rounds, fed_part_prox_losses, marker='o', linestyle='-', color='y', label='FedPartProx')\n",
    "# plt.plot(fed_moon_rounds, fed_moon_losses, marker='o', linestyle='-', color='c', label='FedMoon')\n",
    "# plt.plot(fed_part_moon_rounds, fed_part_moon_losses, marker='o', linestyle='-', color='purple', label='FedPartMoon')\n",
    "\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Aggregate Client Loss for Each Round')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# fed_moon_model_rounds = list(fed_moon_model_no_freeze_results.keys())\n",
    "# fed_moon_accuracies = [fed_moon_model_no_freeze_results[round][\"global_metrics\"][\"accuracy\"] for round in fed_moon_model_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_accuracies, marker='o', linestyle='-', color='b', label='FedAvg')\n",
    "# plt.plot(fed_part_avg_model_rounds, fed_part_avg_accuracies, marker='o', linestyle='-', color='r', label='FedPartAvg')\n",
    "# plt.plot(fed_prox_model_rounds, fed_prox_accuracies, marker='o', linestyle='-', color='g', label='FedProx')\n",
    "# plt.plot(fed_part_prox_model_rounds, fed_part_prox_accuracies, marker='o', linestyle='-', color='y', label='FedPartProx')\n",
    "# plt.plot(fed_moon_model_rounds, fed_moon_accuracies, marker='o', linestyle='-', color='c', label='FedMoon')\n",
    "# plt.plot(fed_part_moon_model_rounds, fed_part_moon_accuracies, marker='o', linestyle='-', color='purple', label='FedPartMoon')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Global Model Accuracy for Each Round')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# fed_moon_global_losses = [fed_moon_model_no_freeze_results[round][\"global_loss\"] for round in fed_moon_model_rounds]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(fed_avg_model_rounds, fed_avg_global_losses, marker='o', linestyle='-', color='b', label='FedAvg')\n",
    "# plt.plot(fed_part_avg_model_rounds, fed_part_avg_global_losses, marker='o', linestyle='-', color='r', label='FedPartAvg')\n",
    "# plt.plot(fed_prox_model_rounds, fed_prox_global_losses, marker='o', linestyle='-', color='g', label='FedProx')\n",
    "# plt.plot(fed_part_prox_model_rounds, fed_part_prox_global_losses, marker='o', linestyle='-', color='y', label='FedPartProx')\n",
    "# plt.plot(fed_moon_model_rounds, fed_moon_global_losses, marker='o', linestyle='-', color='c', label='FedMoon')\n",
    "# plt.plot(fed_part_moon_model_rounds, fed_part_moon_global_losses, marker='o', linestyle='-', color='purple', label='FedPartMoon')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Global Model Loss for Each Round')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
